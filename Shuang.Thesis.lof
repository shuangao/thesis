\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Major procedure for image recognition.\relax }}{3}{figure.caption.3}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Feature extraction using SIFT.\relax }}{4}{figure.caption.4}
\contentsline {figure}{\numberline {1.3}{\ignorespaces General Scheme of Auto Encoders. L1 is the input layer, possibly raw-pixel intensities. L2 is the compressed learned latent representation and L3 is the reconstruction of the given L1 layer from L2 layer. AutoEncoders tries to minimize the difference between L1 and L3 layers with some sparsity constraint.\relax }}{5}{figure.caption.5}
\contentsline {figure}{\numberline {1.4}{\ignorespaces The architecture of ALEXNET (adopted from \cite {krizhevsky2012imagenet}).\relax }}{6}{figure.caption.6}
\contentsline {figure}{\numberline {1.5}{\ignorespaces An intuitive description for human to learn new concept: an okapi can be roughly described as the combination of a body of a horse, legs of the zebra and a head of giraffe.\relax }}{7}{figure.caption.7}
\contentsline {figure}{\numberline {1.6}{\ignorespaces Feature transformation. Transform the data in different domains into a augmented feature space.\relax }}{9}{figure.caption.8}
\contentsline {figure}{\numberline {1.7}{\ignorespaces Difference between two transfer learning scenarios\relax }}{11}{figure.caption.9}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Domain Adaptation}}}{11}{subfigure.7.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Inductive transfer learning the new class}}}{11}{subfigure.7.2}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces One-vs-Rest strategy for multi-class scenario. A three classes problem can be decomposed into 3 binary classification sub-problems.\relax }}{14}{figure.caption.10}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Support Vector Machine\relax }}{17}{figure.caption.11}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Different sparating hyperplanes}}}{17}{subfigure.2.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Max-Margin hyperplanes}}}{17}{subfigure.2.2}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Slack variables for soft-margin SVM\relax }}{18}{figure.caption.12}
\contentsline {figure}{\numberline {2.4}{\ignorespaces The hyperplane of SVM with RBF kernel for non-linear separable data.\relax }}{19}{figure.caption.13}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Apart from the standard machine learning, transfer learning can leverage the information from an additional source: knoweldge from one or more related tasks.\relax }}{23}{figure.caption.14}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Two steps for parameter transfer learning. In the first step multi-source and single source combination are usually used to generate the regularization term. The hyperplane for the transfer model can be obtained by either minimizing training error or cross-validation error on the target training data.\relax }}{25}{figure.caption.18}
\contentsline {figure}{\numberline {2.7}{\ignorespaces Positive transfer VS Negative transfer.\relax }}{27}{figure.caption.19}
\contentsline {figure}{\numberline {2.8}{\ignorespaces Hierarchical Features of Deep Convolutional Neural Networks for face recognition.\relax }}{30}{figure.caption.20}
\contentsline {figure}{\numberline {2.9}{\ignorespaces Projecting $w$ to $w'$ in PMT-SVM (adapted from \cite {aytar2011tabula}).\relax }}{32}{figure.caption.21}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Illustration of feature augmentation in MKTL. $f_i'$ is the output of the $i$-th source model and $\beta _{in}$ is the hyperparameter (need to be estimated) to weigh the augmented feature. $\phi _n(x)$ is augmented feature for the $n$-th binary model.\relax }}{38}{figure.caption.22}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Demonstration of using the source class probability as the auxiliary bias to adjust the output of the target model. The source task is to distinguish the 4 pop cans while the target one is to distinguish the 4 pop bottles\relax }}{39}{figure.caption.23}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Bi-level Optimization problem for EMTLe.\relax }}{41}{figure.caption.24}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Recognition accuracy for HTL domain adaptation from a single source (Part1). 5 different sizes of target training sets are used in each group of experiments. A, D, W and C denote the 4 subsets in Table \ref {tab:class_info} respectively.\relax }}{47}{figure.caption.26}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {A$\rightarrow $D}}}{47}{subfigure.4.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {C$\rightarrow $D}}}{47}{subfigure.4.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {W$\rightarrow $D}}}{47}{subfigure.4.3}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {A$\rightarrow $W}}}{47}{subfigure.4.4}
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {C$\rightarrow $W}}}{47}{subfigure.4.5}
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {D$\rightarrow $W}}}{47}{subfigure.4.6}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Recognition accuracy for HTL domain adaptation from a single source (Part2). 5 different sizes of target training sets are used in each group of experiments. A, D, W and C denote the 4 subsets in Table \ref {tab:class_info} respectively.\relax }}{48}{figure.caption.27}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {C$\rightarrow $A}}}{48}{subfigure.5.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {D$\rightarrow $A}}}{48}{subfigure.5.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {W$\rightarrow $A}}}{48}{subfigure.5.3}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {A$\rightarrow $C}}}{48}{subfigure.5.4}
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {D$\rightarrow $C}}}{48}{subfigure.5.5}
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {W$\rightarrow $C}}}{48}{subfigure.5.6}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Recognition Accuracy for Multi-Model \& Multi-Source experiment on two target datasets. \relax }}{49}{figure.caption.29}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {D+W $\rightarrow $ A}}}{49}{subfigure.6.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {D+W $\rightarrow $ C}}}{49}{subfigure.6.2}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Illustration of Generalized Distillation training process.\relax }}{52}{figure.caption.30}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Illustration of GDSDA training process and our ``fake label" strategy.\relax }}{53}{figure.caption.31}
\contentsline {figure}{\numberline {4.3}{\ignorespaces An example of using GDSDA to generate distilled labels for the target data.\relax }}{55}{figure.caption.32}
\contentsline {figure}{\numberline {4.4}{\ignorespaces D+W$\rightarrow $A, Multi-source results comparison.\relax }}{61}{figure.caption.33}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Experiment results on DSLR$\rightarrow $Amazon and Webcam$\rightarrow $Amazon when there are just one labeled examples per class. The X-axis denotes the imitation parameter of the hard label (i.e. $\lambda _1$ in Fig \ref {fig:GDSDA}) and the corresponding imitation parameter of the soft label is set to $1-\lambda _1$. \relax }}{63}{figure.caption.34}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {D $\rightarrow $ A, 10 unlabeled }}}{63}{subfigure.5.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {D $\rightarrow $ A, 15 unlabeled }}}{63}{subfigure.5.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {D $\rightarrow $ A, 20 unlabeled }}}{63}{subfigure.5.3}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {W $\rightarrow $ A, 10 unlabeled }}}{63}{subfigure.5.4}
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {W $\rightarrow $ A, 15 unlabeled }}}{63}{subfigure.5.5}
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {W $\rightarrow $ A, 20 unlabeled }}}{63}{subfigure.5.6}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Demonstration of Fine-tuning from ImageNet 1000 classes to Food-101 datasets.\relax }}{65}{figure.caption.35}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Convolution operation with $3\times 3$ kernel, stride 1 and padding 1. $\otimes $ denotes the convolutional operator.\relax }}{67}{figure.caption.36}
\contentsline {figure}{\numberline {5.3}{\ignorespaces $2\times 2$ pooling layer with stride 2 and padding 0.\relax }}{68}{figure.caption.37}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Dropout Layers. Adopted from Standford CS231n Convolutional Neural Networks for Visual Recognition\relax }}{69}{figure.caption.38}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces { Standard Neural Net }}}{69}{subfigure.4.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {After Dropout}}}{69}{subfigure.4.2}
\contentsline {figure}{\numberline {5.5}{\ignorespaces Inception Module. $n\times n$ stands for size $n$ receptive field, $n\times n\_reduce$ stands for the $1\times 1$ convolutional layer before the $n\times n$ convolution layer and $pool\_proj$ is another $1\times 1$ convolutional layer after the MAX pooling layer. The output layer concatenates all its input layers.\relax }}{70}{figure.caption.39}
\contentsline {figure}{\numberline {5.6}{\ignorespaces Crop area from original image\relax }}{72}{figure.caption.40}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Original image}}}{72}{subfigure.6.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Center}}}{72}{subfigure.6.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Center mirror}}}{72}{subfigure.6.3}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {Up-left}}}{72}{subfigure.6.4}
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {Up-left mirror}}}{72}{subfigure.6.5}
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {Up-right}}}{72}{subfigure.6.6}
\contentsline {subfigure}{\numberline {(g)}{\ignorespaces {Up-right mirror}}}{72}{subfigure.6.7}
\contentsline {subfigure}{\numberline {(h)}{\ignorespaces {Bottom-left}}}{72}{subfigure.6.8}
\contentsline {subfigure}{\numberline {(i)}{\ignorespaces {Bottom-left mirror}}}{72}{subfigure.6.9}
\contentsline {subfigure}{\numberline {(j)}{\ignorespaces {Bottom-right}}}{72}{subfigure.6.10}
\contentsline {subfigure}{\numberline {(k)}{\ignorespaces {Bottom-right mirror}}}{72}{subfigure.6.11}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Different data augmentation methods\relax }}{74}{figure.caption.41}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Original image}}}{74}{subfigure.7.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Red casting}}}{74}{subfigure.7.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Green casting}}}{74}{subfigure.7.3}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {Blue casting}}}{74}{subfigure.7.4}
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {RGB casting}}}{74}{subfigure.7.5}
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {Vignette}}}{74}{subfigure.7.6}
\contentsline {subfigure}{\numberline {(g)}{\ignorespaces {More vignette}}}{74}{subfigure.7.7}
\contentsline {subfigure}{\numberline {(h)}{\ignorespaces {Horizontal stretch}}}{74}{subfigure.7.8}
\contentsline {subfigure}{\numberline {(i)}{\ignorespaces {More horizontal stretch}}}{74}{subfigure.7.9}
\contentsline {subfigure}{\numberline {(j)}{\ignorespaces {Vertical stretch}}}{74}{subfigure.7.10}
\contentsline {subfigure}{\numberline {(k)}{\ignorespaces {More vertical stretch}}}{74}{subfigure.7.11}
\contentsline {subfigure}{\numberline {(l)}{\ignorespaces {Rotation}}}{74}{subfigure.7.12}
\contentsline {figure}{\numberline {5.8}{\ignorespaces Visualization of some feature maps of different GoogLeNet models in different layers for the same input image. 64 feature maps of each layer are shown. Conv1 is the first convolutional layer and Inception\_5b is the last convolutional layer. \relax }}{76}{figure.caption.46}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
