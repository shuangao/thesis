\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Major procedure for image recognition.\relax }}{2}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Feature extraction using SIFT.\relax }}{4}
\contentsline {figure}{\numberline {1.3}{\ignorespaces General Scheme of Auto Encoders. L1 is the input layer, possibly raw-pixel intensities. L2 is the compressed learned latent representation and L3 is the reconstruction of the given L1 layer from L2 layer. AutoEncoders tries to minimize the difference between L1 and L3 layers with some sparsity constraint.\relax }}{5}
\contentsline {figure}{\numberline {1.4}{\ignorespaces The architecture of ALEXNET (adapted from \cite {krizhevsky2012imagenet}).\relax }}{5}
\contentsline {figure}{\numberline {1.5}{\ignorespaces Different nutrition facts between the burgers in McDonald and home-made.\relax }}{8}
\contentsline {figure}{\numberline {1.6}{\ignorespaces Multi-source category case: an okapi can be roughly described as the combination of a body of a horse, legs of the zebra and a head of giraffe.\relax }}{9}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces One-vs-Rest strategy for multi-class scenario. A three classes problem can be decomposed into 3 binary classification sub-problems.\relax }}{13}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Support Vector Machine\relax }}{16}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Different sparating hyperplanes}}}{16}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Max-Margin hyperplanes}}}{16}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Slack variables for soft-margin SVM\relax }}{17}
\contentsline {figure}{\numberline {2.4}{\ignorespaces The hyperplane of SVM with RBF kernel for non-linear separable data.\relax }}{18}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Apart from the standard machine learning, transfer learning can leverage the information from an additional source: knoweldge from one or more related tasks.\relax }}{20}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Two steps for parameter transfer learning. In the first step multi-source and single source combination are usually used to generate the regularazation term. The hyperplane for the transfer model can be obtained by either minimizing training error or cross-validation error on the target training data.\relax }}{22}
\contentsline {figure}{\numberline {2.7}{\ignorespaces Positive transfer VS Negative transfer.\relax }}{24}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Projecting $w$ to $w'$ in PMT-SVM (adapted from \cite {aytar2011tabula}).\relax }}{29}
\contentsline {figure}{\numberline {3.2}{\ignorespaces A graphical representation of linear argumentation. The score from the source model can be considered as an auxiliary feature and the transfer parameter controls the value of the auxiliary feature. For linear classifiers, it can be considered as a linear combination of the decision from the source model and target data (see \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces 3.15\hbox {}\unskip \@@italiccorr )}}).\relax }}{34}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Suppose the original data is one dimensional and we use the blue line to denote the optimal decision surfce of a linear model (the upper figure). By adding an related auxiliary feature, we can improve the performacne of the classifier (the bottom-left figure) while unrelated one can decrease the performance and lead to negative transfer (the bottom-right figure). \relax }}{36}
\contentsline {figure}{\numberline {3.4}{\ignorespaces An illustration of 5-fold cross-validation\relax }}{40}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Loss function comparision for multi-class hinge loss $\epsilon _{multi}$ and classical zero-one loss $\epsilon _{H}$\relax }}{42}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Illustration of the margin bound for a single example in 3 scenarios. The cirles denote different confidences and the correct label is ploted in dark grey. The height of each lable is the confidence score. In the left figure,$\epsilon (x)=0$. In the middle figure, even though the confidence of the correct label is the largest, it fails to be larger by 1 than the confidence of the runner-up and have a small loss. In the right figure, the confidence of the correct label is not the largest one and have a very large loss.\relax }}{43}
\contentsline {figure}{\numberline {3.7}{\ignorespaces Some examples of MNIST \& USPS.\relax }}{47}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {MNIST}}}{47}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {USPS}}}{47}
\contentsline {figure}{\numberline {3.8}{\ignorespaces Illustration of our training procedure. The data is splitted into 3 sets: a source training set, a small target training set and a large test set. Salt \& pepper noise is added into the source training set in order to generate different source models.\relax }}{48}
\contentsline {figure}{\numberline {3.9}{\ignorespaces Images with different level of noise rate. By adding more slat\&pepper noise, the images become less clear.\relax }}{49}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Noise Rate = 0 }}}{49}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Noise Rate = 0.3}}}{49}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Noise Rate = 0.5}}}{49}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {Noise Rate = 0.8}}}{49}
\addvspace {10\p@ }
\addvspace {10\p@ }
