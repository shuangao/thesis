\relax 
\citation{aytar2011tabula}
\citation{krizhevsky2012imagenet}
\citation{rifkin2004defense}
\citation{tsoumakas2006multi}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:works}{{2}{13}}
\newsmartlabel{sec:works}{{2}{2}}
\newnamelabel{sec:works}{{Related Work}{Related Work}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Classifiers for Image Recognition}{13}}
\newlabel{sec:relat:linear}{{2.1}{13}}
\newsmartlabel{sec:relat:linear}{{2.1}{2}}
\newnamelabel{sec:relat:linear}{{Classifiers for Image Recognition}{Classifiers for Image Recognition}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Binary Classification and Multi-class Classification}{13}}
\citation{lecun1989backpropagation}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces One-vs-Rest strategy for multi-class scenario. A three classes problem can be decomposed into 3 binary classification sub-problems.\relax }}{14}}
\newlabel{fig:related:ovsa}{{2.1}{14}}
\newsmartlabel{fig:related:ovsa}{{2.1}{2}}
\newnamelabel{fig:related:ovsa}{{Binary Classification and Multi-class Classification}{Binary Classification and Multi-class Classification}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Softmax Classifier}{14}}
\newlabel{eq:logistic:loss}{{2.2}{15}}
\newsmartlabel{eq:logistic:loss}{{2.2}{2}}
\newlabel{eq:softmax:loss}{{2.4}{15}}
\newsmartlabel{eq:softmax:loss}{{2.4}{2}}
\citation{johansen1990maximum}
\citation{cristianini2000introduction}
\citation{coates2011analysis}
\citation{schuldt2004recognizing}
\citation{yang2009linear}
\citation{cristianini2000introduction}
\citation{shalev2011pegasos}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Support Vector Machines}{16}}
\newlabel{eq:relation:score}{{2.7}{16}}
\newsmartlabel{eq:relation:score}{{2.7}{2}}
\newlabel{fig:relate:svma}{{2.2a}{17}}
\newsmartlabel{fig:relate:svma}{{2.2a}{2}}
\newnamelabel{fig:relate:svma}{{Hard Margin SVM}{Hard Margin SVM}}
\newlabel{sub@fig:relate:svma}{{(a)}{a}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Support Vector Machine\relax }}{17}}
\newlabel{fig:relate:svm}{{2.2}{17}}
\newsmartlabel{fig:relate:svm}{{2.2}{2}}
\newnamelabel{fig:relate:svm}{{Hard Margin SVM}{Hard Margin SVM}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Different sparating hyperplanes}}}{17}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Max-Margin hyperplanes}}}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3.1}Hard Margin SVM}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3.2}Soft Margin SVM}{17}}
\citation{shalev2011pegasos}
\citation{keerthi2003asymptotic}
\citation{aizerman1964probability}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Slack variables for soft-margin SVM\relax }}{18}}
\newlabel{eq:related:softsvm}{{2.11}{18}}
\newsmartlabel{eq:related:softsvm}{{2.11}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3.3}Kernel SVM}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The hyperplane of SVM with RBF kernel for non-linear separable data.\relax }}{19}}
\newlabel{fig:relate:nonlinear}{{2.4}{19}}
\newsmartlabel{fig:relate:nonlinear}{{2.4}{2}}
\newnamelabel{fig:relate:nonlinear}{{Kernel SVM}{Kernel SVM}}
\newlabel{eq:relation:kernel}{{2.12}{19}}
\newsmartlabel{eq:relation:kernel}{{2.12}{2}}
\newlabel{eq:related:primal}{{2.13}{19}}
\newsmartlabel{eq:related:primal}{{2.13}{2}}
\citation{platt1998sequential}
\citation{hsieh2008dual}
\citation{shalev2011pegasos}
\citation{platt1998sequential}
\citation{lecun1998gradient}
\citation{rosenblatt1958perceptron}
\citation{rosenblatt1962principles}
\citation{ivakhnenko1965cybernetic}
\citation{Schmidhuber14}
\citation{farlow1984self}
\citation{ikeda1976sequential}
\citation{kondo2008multi}
\citation{witczak2006gmdh}
\citation{fukushima1980neocognitron}
\newlabel{eq:related:dual}{{2.14}{20}}
\newsmartlabel{eq:related:dual}{{2.14}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Convolutional Neural Networks}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4.1}Early work with Convolutional Neural Networks}{20}}
\citation{lecun1989backpropagation}
\citation{lecun1998gradient}
\citation{lecun1989backpropagation}
\citation{baldi1993neural}
\citation{le1990handwritten}
\citation{marc2006efficient}
\citation{chellapilla2006high}
\citation{ciresan2012multi}
\citation{krizhevsky2012imagenet}
\citation{zeiler2014visualizing}
\citation{simonyan2014very}
\citation{szegedy2014going}
\citation{wu2015deep}
\citation{malmaud2015s}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4.2}Recent achievements with Convlutional Neural Networks}{21}}
\citation{zeiler2014visualizing}
\citation{Chatfield14}
\citation{NIPS2014_Zhou}
\citation{hoffman2013one}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}An Overview of Visual Transfer Learning}{22}}
\citation{pan2010survey}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Apart from the standard machine learning, transfer learning can leverage the information from an additional source: knoweldge from one or more related tasks.\relax }}{23}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Categories of our learning scenarios\relax }}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Types of Transfer Learning from the Situations of Tasks}{23}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Relationship between traditional machine learning and different transfer learning settings\relax }}{23}}
\newlabel{tab:related:transfercmp}{{2.2}{23}}
\newsmartlabel{tab:related:transfercmp}{{2.2}{2}}
\newnamelabel{tab:related:transfercmp}{{Types of Transfer Learning from the Situations of Tasks}{Types of Transfer Learning from the Situations of Tasks}}
\citation{dai2007boosting}
\citation{jiang2007instance}
\citation{liao2005logistic}
\citation{ben2010theory}
\citation{evgeniou2007multi}
\citation{jie2011multiclass}
\citation{daume2009frustratingly}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Various settings of transfer learning\relax }}{24}}
\newlabel{tab:related:transfersetting}{{2.3}{24}}
\newsmartlabel{tab:related:transfersetting}{{2.3}{2}}
\newnamelabel{tab:related:transfersetting}{{Types of Transfer Learning from the Situations of Tasks}{Types of Transfer Learning from the Situations of Tasks}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Types of Transfer Learning from the Aspect of Source Knowledge}{24}}
\citation{evgeniou2004regularized}
\citation{aytar2011tabula}
\citation{tommasi2010safety}
\citation{yang2007cross}
\citation{yosinski2014transferable}
\citation{Chatfield14}
\citation{hoffman2013one}
\citation{zeiler2014visualizing}
\citation{NIPS2014_Zhou}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Two steps for parameter transfer learning. In the first step multi-source and single source combination are usually used to generate the regularization term. The hyperplane for the transfer model can be obtained by either minimizing training error or cross-validation error on the target training data.\relax }}{25}}
\citation{fei2006one}
\citation{rosenstein2005transfer}
\citation{pan2010survey}
\citation{torrey2009transfer}
\citation{lu2015transfer}
\citation{pan2010survey}
\citation{torrey2009transfer}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Special Issues in Avoiding Negative Transfer}{26}}
\citation{torrey2005using}
\citation{rosenstein2005transfer}
\citation{talvitie2007experts}
\citation{kuhlmann2007graph}
\citation{bakker2003task}
\citation{tommasi2014learning}
\citation{jie2011multiclass}
\citation{kuzborskij2013n}
\citation{kuzborskij2013stability}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Positive transfer VS Negative transfer.\relax }}{27}}
\citation{kuzborskij2013stability}
\citation{tommasi2010safety}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Related Work in Absence of the Source Data}{28}}
\citation{farabet2013learning}
\citation{zeiler2014visualizing}
\citation{Chatfield14}
\citation{agrawal2014analyzing}
\citation{hoffman2013one}
\citation{NIPS2014_Zhou}
\citation{yosinski2014transferable}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Fine-tuning the Deep Net}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Hypothesis Transfer Learning with SVMs}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Hierarchical Features of Deep Convolutional Neural Networks for face recognition.\relax }}{30}}
\citation{suykens1999least}
\citation{yang2007adapting}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2.1}LS-SVM Classifier}{31}}
\newlabel{eq:gama:lssvm}{{2.15}{31}}
\newsmartlabel{eq:gama:lssvm}{{2.15}{2}}
\newlabel{sq:gama:lsprime}{{2.16}{31}}
\newsmartlabel{sq:gama:lsprime}{{2.16}{2}}
\newlabel{eq:single:orgmatrix}{{2.17}{31}}
\newsmartlabel{eq:single:orgmatrix}{{2.17}{2}}
\newlabel{eq:gama:psi}{{2.18}{31}}
\newsmartlabel{eq:gama:psi}{{2.18}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2.2}ASVM \& PMT-SVM}{31}}
\citation{aytar2011tabula}
\citation{aytar2011tabula}
\citation{aytar2011tabula}
\citation{tommasi2014learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Projecting $w$ to $w'$ in PMT-SVM (adapted from \cite  {aytar2011tabula}).\relax }}{32}}
\newlabel{fig:gama:pmt}{{2.9}{32}}
\newsmartlabel{fig:gama:pmt}{{2.9}{2}}
\newnamelabel{fig:gama:pmt}{{ASVM \& PMT-SVM}{ASVM \& PMT-SVM}}
\newlabel{eq:gama:asvm}{{2.20}{32}}
\newsmartlabel{eq:gama:asvm}{{2.20}{2}}
\newlabel{eq:gama:pmt}{{2.21}{32}}
\newsmartlabel{eq:gama:pmt}{{2.21}{2}}
\citation{cawley2006leave}
\newlabel{eq:gama:multi}{{2.22}{33}}
\newsmartlabel{eq:gama:multi}{{2.22}{2}}
\newlabel{eq:gama:loo}{{2.24}{33}}
\newsmartlabel{eq:gama:loo}{{2.24}{2}}
\newlabel{eq:gama:multib}{{2.25}{33}}
\newsmartlabel{eq:gama:multib}{{2.25}{2}}
\citation{kuzborskij2013n}
\citation{crammer2002algorithmic}
\citation{BoydCO}
\citation{hinton2015distilling}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2.3}MULTIpLE}{34}}
\newlabel{eq:gama:multiple}{{2.26}{34}}
\newsmartlabel{eq:gama:multiple}{{2.26}{2}}
\newlabel{eq:gama:multib}{{2.27}{34}}
\newsmartlabel{eq:gama:multib}{{2.27}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Distillation for Knowledge Transfer}{34}}
\citation{vapnik1999overview}
\citation{Tzeng_2015_ICCV}
\citation{urban2016deep}
\citation{luo2016face}
\citation{Gupta_2016_CVPR}
\newlabel{eq:rw:general}{{2.28}{35}}
\newsmartlabel{eq:rw:general}{{2.28}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Summary}{36}}
\@setckpt{relatedwork/total}{
\setcounter{page}{37}
\setcounter{equation}{28}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{9}
\setcounter{table}{3}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{parentequation}{0}
\setcounter{@smartlistlen}{2}
\setcounter{less@smartlist}{0}
\setcounter{@currsmartlistplace}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{2}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lips@count}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{defi}{0}
\setcounter{theorem}{0}
\setcounter{myappendices}{0}
\setcounter{appdepth}{1}
}
