\relax 
\citation{aytar2011tabula}
\citation{krizhevsky2012imagenet}
\citation{rifkin2004defense}
\citation{tsoumakas2006multi}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Classifiers for Image Recognition}{11}}
\newlabel{sec:relat:linear}{{2.1}{11}}
\newsmartlabel{sec:relat:linear}{{2.1}{2}}
\newnamelabel{sec:relat:linear}{{Classifiers for Image Recognition}{Classifiers for Image Recognition}}
\citation{lecun1989backpropagation}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces One-vs-Rest strategy for multi-class scenario. A three classes problem can be decomposed into 3 binary classification sub-problems.\relax }}{12}}
\newlabel{fig:related:ovsa}{{2.1}{12}}
\newsmartlabel{fig:related:ovsa}{{2.1}{2}}
\newnamelabel{fig:related:ovsa}{{Classifiers for Image Recognition}{Classifiers for Image Recognition}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Linear Method: Softmax Classifier}{12}}
\citation{johansen1990maximum}
\newlabel{eq:logistic:loss}{{2.2}{13}}
\newsmartlabel{eq:logistic:loss}{{2.2}{2}}
\newlabel{eq:softmax:loss}{{2.4}{13}}
\newsmartlabel{eq:softmax:loss}{{2.4}{2}}
\citation{cristianini2000introduction}
\citation{coates2011analysis}
\citation{schuldt2004recognizing}
\citation{yang2009linear}
\citation{cristianini2000introduction}
\citation{shalev2011pegasos}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Kernel Method: Support Vector Machines}{14}}
\newlabel{eq:relation:score}{{2.7}{14}}
\newsmartlabel{eq:relation:score}{{2.7}{2}}
\@writefile{toc}{\contentsline {subsubsection}{Hard Margin SVM}{14}}
\newlabel{fig:relate:svma}{{2.2a}{15}}
\newsmartlabel{fig:relate:svma}{{2.2a}{2}}
\newnamelabel{fig:relate:svma}{{Hard Margin SVM}{Hard Margin SVM}}
\newlabel{sub@fig:relate:svma}{{(a)}{a}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Support Vector Machine\relax }}{15}}
\newlabel{fig:relate:svm}{{2.2}{15}}
\newsmartlabel{fig:relate:svm}{{2.2}{2}}
\newnamelabel{fig:relate:svm}{{Hard Margin SVM}{Hard Margin SVM}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Different sparating hyperplanes}}}{15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Max-Margin hyperplanes}}}{15}}
\@writefile{toc}{\contentsline {subsubsection}{Soft Margin SVM}{15}}
\citation{shalev2011pegasos}
\citation{keerthi2003asymptotic}
\citation{aizerman1964probability}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Slack variables for soft-margin SVM\relax }}{16}}
\newlabel{eq:related:softsvm}{{2.11}{16}}
\newsmartlabel{eq:related:softsvm}{{2.11}{2}}
\@writefile{toc}{\contentsline {subsubsection}{Kernel SVM}{16}}
\newlabel{eq:relation:kernel}{{2.12}{16}}
\newsmartlabel{eq:relation:kernel}{{2.12}{2}}
\citation{platt1998sequential}
\citation{hsieh2008dual}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The hyperplane of SVM with RBF kernel for non-linear separable data.\relax }}{17}}
\newlabel{fig:relate:nonlinear}{{2.4}{17}}
\newsmartlabel{fig:relate:nonlinear}{{2.4}{2}}
\newnamelabel{fig:relate:nonlinear}{{Kernel SVM}{Kernel SVM}}
\newlabel{eq:related:primal}{{2.13}{17}}
\newsmartlabel{eq:related:primal}{{2.13}{2}}
\newlabel{eq:related:dual}{{2.14}{17}}
\newsmartlabel{eq:related:dual}{{2.14}{2}}
\citation{shalev2011pegasos}
\citation{platt1998sequential}
\citation{rosenblatt1958perceptron}
\citation{rosenblatt1962principles}
\citation{ivakhnenko1965cybernetic}
\citation{Schmidhuber14}
\citation{farlow1984self}
\citation{ikeda1976sequential}
\citation{kondo2008multi}
\citation{witczak2006gmdh}
\citation{fukushima1980neocognitron}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Convlutional Neural Networks}{18}}
\@writefile{toc}{\contentsline {subsubsection}{Early work with Convlutional Neural Networks}{18}}
\citation{lecun1989backpropagation}
\citation{baldi1993neural}
\citation{le1990handwritten}
\citation{marc2006efficient}
\citation{chellapilla2006high}
\citation{ciresan2012multi}
\citation{krizhevsky2012imagenet}
\citation{zeiler2014visualizing}
\citation{simonyan2014very}
\citation{szegedy2014going}
\citation{wu2015deep}
\citation{malmaud2015s}
\@writefile{toc}{\contentsline {subsubsection}{Recent achievements with Convlutional Neural Networks}{19}}
\citation{zeiler2014visualizing}
\citation{Chatfield14}
\citation{NIPS2014_Zhou}
\citation{hoffman2013one}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Apart from the standard machine learning, transfer learning can leverage the information from an additional source: knoweldge from one or more related tasks.\relax }}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Visual Transfer Learning without the source data}{20}}
\citation{pan2010survey}
\citation{pan2010survey}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Relationship between traditional machine learning and different transfer learning settings\relax }}{21}}
\newlabel{tab:related:transfercmp}{{2.1}{21}}
\newsmartlabel{tab:related:transfercmp}{{2.1}{2}}
\newnamelabel{tab:related:transfercmp}{{Visual Transfer Learning without the source data}{Visual Transfer Learning without the source data}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Various settings of transfer learning\relax }}{21}}
\newlabel{tab:related:transfersetting}{{2.2}{21}}
\newsmartlabel{tab:related:transfersetting}{{2.2}{2}}
\newnamelabel{tab:related:transfersetting}{{Visual Transfer Learning without the source data}{Visual Transfer Learning without the source data}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Inductive Transfer Learning}{21}}
\citation{dai2007boosting}
\citation{jiang2007instance}
\citation{liao2005logistic}
\citation{ben2010theory}
\citation{evgeniou2007multi}
\citation{jie2011multiclass}
\citation{daume2009frustratingly}
\citation{evgeniou2004regularized}
\citation{aytar2011tabula}
\citation{tommasi2010safety}
\citation{yang2007cross}
\citation{yosinski2014transferable}
\citation{Chatfield14}
\citation{hoffman2013one}
\citation{zeiler2014visualizing}
\citation{NIPS2014_Zhou}
\citation{fei2006one}
\citation{rosenstein2005transfer}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Two steps for parameter transfer learning. In the first step multi-source and single source combination are usually used to generate the regularazation term. The hyperplane for the transfer model can be obtained by either minimizing training error or cross-validation error on the target training data.\relax }}{23}}
\citation{suykens1999least}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Fine-tuning the Deep Net}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Hypothesis Transfer Learning}{24}}
\newlabel{eq:gama:lssvm}{{2.15}{24}}
\newsmartlabel{eq:gama:lssvm}{{2.15}{2}}
\newlabel{sq:gama:lsprime}{{2.16}{24}}
\newsmartlabel{sq:gama:lsprime}{{2.16}{2}}
\newlabel{eq:single:orgmatrix}{{2.17}{24}}
\newsmartlabel{eq:single:orgmatrix}{{2.17}{2}}
\newlabel{eq:gama:psi}{{2.18}{24}}
\newsmartlabel{eq:gama:psi}{{2.18}{2}}
\citation{yang2007adapting}
\citation{aytar2011tabula}
\citation{aytar2011tabula}
\citation{aytar2011tabula}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Projecting $w$ to $w'$ in PMT-SVM (adapted from \cite  {aytar2011tabula}).\relax }}{25}}
\newlabel{fig:gama:pmt}{{2.7}{25}}
\newsmartlabel{fig:gama:pmt}{{2.7}{2}}
\newnamelabel{fig:gama:pmt}{{Hypothesis Transfer Learning}{Hypothesis Transfer Learning}}
\newlabel{eq:gama:asvm}{{2.20}{25}}
\newsmartlabel{eq:gama:asvm}{{2.20}{2}}
\citation{tommasi2014learning}
\citation{cawley2006leave}
\newlabel{eq:gama:pmt}{{2.21}{26}}
\newsmartlabel{eq:gama:pmt}{{2.21}{2}}
\newlabel{eq:gama:multi}{{2.22}{26}}
\newsmartlabel{eq:gama:multi}{{2.22}{2}}
\citation{kuzborskij2013n}
\citation{crammer2002algorithmic}
\citation{BoydCO}
\citation{pan2010survey}
\newlabel{eq:gama:loo}{{2.24}{27}}
\newsmartlabel{eq:gama:loo}{{2.24}{2}}
\newlabel{eq:gama:multib}{{2.25}{27}}
\newsmartlabel{eq:gama:multib}{{2.25}{2}}
\newlabel{eq:gama:multiple}{{2.26}{27}}
\newsmartlabel{eq:gama:multiple}{{2.26}{2}}
\newlabel{eq:gama:multib}{{2.27}{27}}
\newsmartlabel{eq:gama:multib}{{2.27}{2}}
\citation{torrey2009transfer}
\citation{lu2015transfer}
\citation{pan2010survey}
\citation{torrey2009transfer}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Positive transfer VS Negative transfer.\relax }}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Special Issues in Avoiding Negative Transfer}{28}}
\citation{torrey2005using}
\citation{rosenstein2005transfer}
\citation{talvitie2007experts}
\citation{kuhlmann2007graph}
\citation{bakker2003task}
\citation{tommasi2014learning}
\citation{jie2011multiclass}
\citation{kuzborskij2013n}
\citation{kuzborskij2013stability}
\citation{tommasi2010safety}
\citation{kuzborskij2013stability}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Summary}{30}}
\@setckpt{relatedwork/total}{
\setcounter{page}{31}
\setcounter{equation}{27}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{8}
\setcounter{table}{2}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{parentequation}{0}
\setcounter{@smartlistlen}{2}
\setcounter{less@smartlist}{0}
\setcounter{@currsmartlistplace}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{2}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lips@count}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{defi}{0}
\setcounter{theorem}{0}
\setcounter{myappendices}{0}
\setcounter{appdepth}{1}
}
