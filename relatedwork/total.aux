\relax 
\citation{aytar2011tabula}
\citation{krizhevsky2012imagenet}
\citation{rifkin2004defense}
\citation{tsoumakas2006multi}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{12}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Classifiers for Image Recognition}{12}}
\newlabel{sec:relat:linear}{{2.1}{12}}
\newsmartlabel{sec:relat:linear}{{2.1}{2}}
\newnamelabel{sec:relat:linear}{{Classifiers for Image Recognition}{Classifiers for Image Recognition}}
\citation{lecun1989backpropagation}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces One-vs-Rest strategy for multi-class scenario. A three classes problem can be decomposed into 3 binary classification sub-problems.\relax }}{13}}
\newlabel{fig:related:ovsa}{{2.1}{13}}
\newsmartlabel{fig:related:ovsa}{{2.1}{2}}
\newnamelabel{fig:related:ovsa}{{Classifiers for Image Recognition}{Classifiers for Image Recognition}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Linear Method: Softmax Classifier}{13}}
\citation{johansen1990maximum}
\newlabel{eq:logistic:loss}{{2.2}{14}}
\newsmartlabel{eq:logistic:loss}{{2.2}{2}}
\newlabel{eq:softmax:loss}{{2.4}{14}}
\newsmartlabel{eq:softmax:loss}{{2.4}{2}}
\citation{cristianini2000introduction}
\citation{coates2011analysis}
\citation{schuldt2004recognizing}
\citation{yang2009linear}
\citation{cristianini2000introduction}
\citation{shalev2011pegasos}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Kernel Method: Support Vector Machines}{15}}
\newlabel{eq:relation:score}{{2.7}{15}}
\newsmartlabel{eq:relation:score}{{2.7}{2}}
\@writefile{toc}{\contentsline {subsubsection}{Hard Margin SVM}{15}}
\newlabel{fig:relate:svma}{{2.2a}{16}}
\newsmartlabel{fig:relate:svma}{{2.2a}{2}}
\newnamelabel{fig:relate:svma}{{Hard Margin SVM}{Hard Margin SVM}}
\newlabel{sub@fig:relate:svma}{{(a)}{a}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Support Vector Machine\relax }}{16}}
\newlabel{fig:relate:svm}{{2.2}{16}}
\newsmartlabel{fig:relate:svm}{{2.2}{2}}
\newnamelabel{fig:relate:svm}{{Hard Margin SVM}{Hard Margin SVM}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Different sparating hyperplanes}}}{16}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Max-Margin hyperplanes}}}{16}}
\@writefile{toc}{\contentsline {subsubsection}{Soft Margin SVM}{16}}
\citation{shalev2011pegasos}
\citation{keerthi2003asymptotic}
\citation{aizerman1964probability}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Slack variables for soft-margin SVM\relax }}{17}}
\newlabel{eq:related:softsvm}{{2.11}{17}}
\newsmartlabel{eq:related:softsvm}{{2.11}{2}}
\@writefile{toc}{\contentsline {subsubsection}{Kernel SVM}{17}}
\newlabel{eq:relation:kernel}{{2.12}{17}}
\newsmartlabel{eq:relation:kernel}{{2.12}{2}}
\citation{platt1998sequential}
\citation{hsieh2008dual}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The hyperplane of SVM with RBF kernel for non-linear separable data.\relax }}{18}}
\newlabel{fig:relate:nonlinear}{{2.4}{18}}
\newsmartlabel{fig:relate:nonlinear}{{2.4}{2}}
\newnamelabel{fig:relate:nonlinear}{{Kernel SVM}{Kernel SVM}}
\newlabel{eq:related:primal}{{2.13}{18}}
\newsmartlabel{eq:related:primal}{{2.13}{2}}
\newlabel{eq:related:dual}{{2.14}{18}}
\newsmartlabel{eq:related:dual}{{2.14}{2}}
\citation{shalev2011pegasos}
\citation{platt1998sequential}
\citation{pan2010survey}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Transfer Learning}{19}}
\citation{pan2010survey}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Apart from the standard machine learning, transfer learning can leverage the information from an additional source: knoweldge from one or more related tasks.\relax }}{20}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Relationship between traditional machine learning and different transfer learning settings\relax }}{20}}
\newlabel{tab:related:transfercmp}{{2.1}{20}}
\newsmartlabel{tab:related:transfercmp}{{2.1}{2}}
\newnamelabel{tab:related:transfercmp}{{Transfer Learning}{Transfer Learning}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Inductive Transfer Learning}{20}}
\citation{dai2007boosting}
\citation{jiang2007instance}
\citation{liao2005logistic}
\citation{ben2010theory}
\citation{evgeniou2007multi}
\citation{jie2011multiclass}
\citation{daume2009frustratingly}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Various settings of transfer learning\relax }}{21}}
\newlabel{tab:related:transfersetting}{{2.2}{21}}
\newsmartlabel{tab:related:transfersetting}{{2.2}{2}}
\newnamelabel{tab:related:transfersetting}{{Transfer Learning}{Transfer Learning}}
\citation{evgeniou2004regularized}
\citation{aytar2011tabula}
\citation{tommasi2010safety}
\citation{yang2007cross}
\citation{yosinski2014transferable}
\citation{Chatfield14}
\citation{hoffman2013one}
\citation{zeiler2014visualizing}
\citation{NIPS2014_Zhou}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Two steps for parameter transfer learning. In the first step multi-source and single source combination are usually used to generate the regularazation term. The hyperplane for the transfer model can be obtained by either minimizing training error or cross-validation error on the target training data.\relax }}{22}}
\citation{fei2006one}
\citation{rosenstein2005transfer}
\citation{pan2010survey}
\citation{torrey2009transfer}
\citation{lu2015transfer}
\citation{pan2010survey}
\citation{torrey2009transfer}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Avoid Negative Transfer}{23}}
\citation{torrey2005using}
\citation{rosenstein2005transfer}
\citation{talvitie2007experts}
\citation{kuhlmann2007graph}
\citation{bakker2003task}
\citation{tommasi2014learning}
\citation{jie2011multiclass}
\citation{kuzborskij2013n}
\citation{kuzborskij2013stability}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Positive transfer VS Negative transfer.\relax }}{24}}
\citation{tommasi2010safety}
\citation{kuzborskij2013stability}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Summary}{25}}
\@setckpt{relatedwork/total}{
\setcounter{page}{26}
\setcounter{equation}{14}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{7}
\setcounter{table}{2}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{parentequation}{0}
\setcounter{@smartlistlen}{2}
\setcounter{less@smartlist}{0}
\setcounter{@currsmartlistplace}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{2}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lips@count}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{theorem}{0}
\setcounter{myappendices}{0}
\setcounter{appdepth}{1}
}
