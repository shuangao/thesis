\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{aytar2011tabula}
\citation{krizhevsky2012imagenet}
\citation{rifkin2004defense}
\citation{tsoumakas2006multi}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{13}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:works}{{2}{13}{Related Work}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Classifiers for Image Recognition}{13}{section.2.1}}
\newlabel{sec:relat:linear}{{2.1}{13}{Classifiers for Image Recognition}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Binary Classification and Multi-class Classification}{13}{subsection.2.1.1}}
\citation{vapnik1999overview}
\citation{lecun1989backpropagation}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces One-vs-Rest strategy for multi-class scenario. A three classes problem can be decomposed into 3 binary classification sub-problems.\relax }}{14}{figure.caption.10}}
\newlabel{fig:related:ovsa}{{2.1}{14}{One-vs-Rest strategy for multi-class scenario. A three classes problem can be decomposed into 3 binary classification sub-problems.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Softmax Classifier}{14}{subsection.2.1.2}}
\newlabel{eq:logistic:loss}{{2.2}{15}{Softmax Classifier}{equation.2.1.2}{}}
\newlabel{eq:softmax:loss}{{2.4}{15}{Softmax Classifier}{equation.2.1.4}{}}
\citation{johansen1990maximum}
\citation{cristianini2000introduction}
\citation{coates2011analysis}
\citation{schuldt2004recognizing}
\citation{yang2009linear}
\citation{cristianini2000introduction}
\citation{shalev2011pegasos}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Support Vector Machines}{16}{subsection.2.1.3}}
\newlabel{eq:relation:score}{{2.7}{16}{Support Vector Machines}{equation.2.1.7}{}}
\newlabel{fig:relate:svma}{{2.2a}{17}{Subfigure 2 2.2a}{subfigure.2.2.1}{}}
\newlabel{sub@fig:relate:svma}{{(a)}{a}{Subfigure 2 2.2a\relax }{subfigure.2.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Support Vector Machine\relax }}{17}{figure.caption.11}}
\newlabel{fig:relate:svm}{{2.2}{17}{Support Vector Machine\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Different sparating hyperplanes}}}{17}{subfigure.2.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Max-Margin hyperplanes}}}{17}{subfigure.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3.1}Hard Margin SVM}{17}{subsubsection.2.1.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3.2}Soft Margin SVM}{17}{subsubsection.2.1.3.2}}
\citation{shalev2011pegasos}
\citation{keerthi2003asymptotic}
\citation{aizerman1964probability}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Slack variables for soft-margin SVM\relax }}{18}{figure.caption.12}}
\newlabel{eq:related:softsvm}{{2.11}{18}{Soft Margin SVM}{equation.2.1.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3.3}Kernel SVM}{18}{subsubsection.2.1.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The hyperplane of SVM with RBF kernel for non-linear separable data.\relax }}{19}{figure.caption.13}}
\newlabel{fig:relate:nonlinear}{{2.4}{19}{The hyperplane of SVM with RBF kernel for non-linear separable data.\relax }{figure.caption.13}{}}
\newlabel{eq:relation:kernel}{{2.12}{19}{Kernel SVM}{equation.2.1.12}{}}
\newlabel{eq:related:primal}{{2.13}{19}{Kernel SVM}{equation.2.1.13}{}}
\citation{platt1998sequential}
\citation{hsieh2008dual}
\citation{shalev2011pegasos}
\citation{platt1998sequential}
\citation{lecun1998gradient}
\citation{rosenblatt1958perceptron}
\citation{rosenblatt1962principles}
\citation{ivakhnenko1965cybernetic}
\citation{Schmidhuber14}
\citation{farlow1984self}
\citation{ikeda1976sequential}
\citation{kondo2008multi}
\citation{witczak2006gmdh}
\citation{fukushima1980neocognitron}
\newlabel{eq:related:dual}{{2.14}{20}{Kernel SVM}{equation.2.1.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Convolutional Neural Networks}{20}{subsection.2.1.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4.1}Early Work with Convolutional Neural Networks}{20}{subsubsection.2.1.4.1}}
\citation{lecun1989backpropagation}
\citation{lecun1998gradient}
\citation{lecun1989backpropagation}
\citation{baldi1993neural}
\citation{le1990handwritten}
\citation{marc2006efficient}
\citation{chellapilla2006high}
\citation{ciresan2012multi}
\citation{krizhevsky2012imagenet}
\citation{zeiler2014visualizing}
\citation{simonyan2014very}
\citation{szegedy2014going}
\citation{wu2015deep}
\citation{malmaud2015s}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4.2}Recent Achievements with Convolutional Neural Networks}{21}{subsubsection.2.1.4.2}}
\citation{zeiler2014visualizing}
\citation{Chatfield14}
\citation{NIPS2014_Zhou}
\citation{hoffman2013one}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}An Overview of Visual Transfer Learning}{22}{section.2.2}}
\citation{pan2010survey}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Apart from the standard machine learning, transfer learning can leverage the information from an additional source: knoweldge from one or more related tasks.\relax }}{23}{figure.caption.14}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Categories of our learning scenarios\relax }}{23}{table.caption.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Types of Transfer Learning from the Situations of Tasks}{23}{subsection.2.2.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Relationship between traditional machine learning and different transfer learning settings\relax }}{23}{table.caption.16}}
\newlabel{tab:related:transfercmp}{{2.2}{23}{Relationship between traditional machine learning and different transfer learning settings\relax }{table.caption.16}{}}
\citation{dai2007boosting}
\citation{jiang2007instance}
\citation{liao2005logistic}
\citation{ben2010theory}
\citation{evgeniou2007multi}
\citation{jie2011multiclass}
\citation{daume2009frustratingly}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Various settings of transfer learning\relax }}{24}{table.caption.17}}
\newlabel{tab:related:transfersetting}{{2.3}{24}{Various settings of transfer learning\relax }{table.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Types of Transfer Learning from the Aspect of Source Knowledge}{24}{subsection.2.2.2}}
\citation{evgeniou2004regularized}
\citation{aytar2011tabula}
\citation{tommasi2010safety}
\citation{yang2007cross}
\citation{yosinski2014transferable}
\citation{Chatfield14}
\citation{hoffman2013one}
\citation{zeiler2014visualizing}
\citation{NIPS2014_Zhou}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Two steps for parameter transfer learning. In the first step multi-source and single source combination are usually used to generate the regularization term. The hyperplane for the transfer model can be obtained by either minimizing training error or cross-validation error on the target training data.\relax }}{25}{figure.caption.18}}
\citation{fei2006one}
\citation{rosenstein2005transfer}
\citation{pan2010survey}
\citation{torrey2009transfer}
\citation{lu2015transfer}
\citation{pan2010survey}
\citation{torrey2009transfer}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Special Issues in Avoiding Negative Transfer}{26}{subsection.2.2.3}}
\citation{torrey2005using}
\citation{rosenstein2005transfer}
\citation{talvitie2007experts}
\citation{kuhlmann2007graph}
\citation{bakker2003task}
\citation{tommasi2014learning}
\citation{jie2011multiclass}
\citation{kuzborskij2013n}
\citation{kuzborskij2013stability}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Positive transfer VS Negative transfer.\relax }}{27}{figure.caption.19}}
\citation{kuzborskij2013stability}
\citation{tommasi2010safety}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Related Work in Hypothesis Transfer Learning}{28}{section.2.3}}
\citation{farabet2013learning}
\citation{zeiler2014visualizing}
\citation{Chatfield14}
\citation{agrawal2014analyzing}
\citation{hoffman2013one}
\citation{NIPS2014_Zhou}
\citation{yosinski2014transferable}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Fine-tuning the Deep Net}{29}{subsection.2.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Hypothesis Transfer Learning with SVMs}{29}{subsection.2.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Hierarchical Features of Deep Convolutional Neural Networks for face recognition.\relax }}{30}{figure.caption.20}}
\citation{suykens1999least}
\citation{yang2007adapting}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2.1}LS-SVM Classifier}{31}{subsubsection.2.3.2.1}}
\newlabel{eq:gama:lssvm}{{2.15}{31}{LS-SVM Classifier}{equation.2.3.15}{}}
\newlabel{sq:gama:lsprime}{{2.16}{31}{LS-SVM Classifier}{equation.2.3.16}{}}
\newlabel{eq:single:orgmatrix}{{2.17}{31}{LS-SVM Classifier}{equation.2.3.17}{}}
\newlabel{eq:gama:psi}{{2.18}{31}{LS-SVM Classifier}{equation.2.3.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2.2}ASVM \& PMT-SVM}{31}{subsubsection.2.3.2.2}}
\citation{aytar2011tabula}
\citation{aytar2011tabula}
\citation{aytar2011tabula}
\citation{tommasi2014learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Projecting $w$ to $w'$ in PMT-SVM (adapted from \cite  {aytar2011tabula}).\relax }}{32}{figure.caption.21}}
\newlabel{fig:gama:pmt}{{2.9}{32}{Projecting $w$ to $w'$ in PMT-SVM (adapted from \cite {aytar2011tabula}).\relax }{figure.caption.21}{}}
\newlabel{eq:gama:asvm}{{2.20}{32}{ASVM \& PMT-SVM}{equation.2.3.20}{}}
\newlabel{eq:gama:pmt}{{2.21}{32}{ASVM \& PMT-SVM}{equation.2.3.21}{}}
\citation{cawley2006leave}
\newlabel{eq:gama:multi}{{2.22}{33}{ASVM \& PMT-SVM}{equation.2.3.22}{}}
\newlabel{eq:gama:loo}{{2.24}{33}{ASVM \& PMT-SVM}{equation.2.3.24}{}}
\newlabel{eq:gama:multib}{{2.25}{33}{ASVM \& PMT-SVM}{equation.2.3.25}{}}
\citation{kuzborskij2013n}
\citation{crammer2002algorithmic}
\citation{BoydCO}
\citation{hinton2015distilling}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2.3}MULTIpLE}{34}{subsubsection.2.3.2.3}}
\newlabel{eq:gama:multiple}{{2.26}{34}{MULTIpLE}{equation.2.3.26}{}}
\newlabel{eq:gama:multib}{{2.27}{34}{MULTIpLE}{equation.2.3.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Distillation for Knowledge Transfer}{34}{subsection.2.3.3}}
\citation{vapnik1999overview}
\citation{Tzeng_2015_ICCV}
\citation{urban2016deep}
\citation{luo2016face}
\citation{Gupta_2016_CVPR}
\newlabel{eq:rw:general}{{2.28}{35}{Distillation for Knowledge Transfer}{equation.2.3.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Summary}{36}{section.2.4}}
\@setckpt{relatedwork/total}{
\setcounter{page}{37}
\setcounter{equation}{28}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{9}
\setcounter{table}{3}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{parentequation}{0}
\setcounter{@smartlistlen}{2}
\setcounter{less@smartlist}{0}
\setcounter{@currsmartlistplace}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{2}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lips@count}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{defi}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{1}
\setcounter{bookmark@seq@number}{44}
\setcounter{theorem}{0}
\setcounter{myappendices}{0}
\setcounter{appdepth}{1}
\setcounter{section@level}{1}
}
