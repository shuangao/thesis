\textbf{Distillation} \cite{hinton2015distilling} was developed frameworks for knowledge transfer and addressed the problem how to effectively transfer the knowledge from the source model directly. In chapter \ref{sec:aaai}, we propose a framework called GDSDA based on it to solve semi-supervised domain adaptation problem. In this part, we review the the principle of Distillation and some related work using this framework. The technical details will be introduced in chapter \ref{sec:aaai}.

Hinton et al. proposed Distillation to transfer the knowledge from a source neural network (or a whole ensemble of neural networks) to a single target one. In this setting, the capacity of the source neural network is large while the capacity of the target one is small. The capacity reflects the expressive power of a model and a model with larger capacity can fit complex data better. In statistical learning theory \cite{vapnik1999overview}, the relationship of the generalization error $e_{te}$ and training error $e_{tr}$ of a model can be expressed as follow:
\begin{equation}\label{eq:rw:general}
e_{te}\leq e_{tr}+2\sqrt{\frac{\log h +\log\frac{2}{\eta}}{2N}}
\end{equation}
Here, $h$ is the capacity (VC dimension) of the model and $N$ is the training set size. Inequation \ref{eq:rw:general} holds with a probability of $1-\eta$.
When we train the target model, we can let the it mimick the output of the source one on the training set. If both source and target models can achieve similar training error on the training set, the small target model can typically do much better on the test data due to its lower capacity. In this process, we actually don't need the true label of the training data. Instead, we only require the outputs of the source model of the training data. However, introducing the true label of the training data can further improve the performance of the target model.

Distillation is typically used for training the deep neural network for knowledge transfer between different models and tasks. Tzeng et al. \cite{Tzeng_2015_ICCV} proposed a CNN architecture for domain adaptation to leverage the knowledge from limited or no labeled data using the soft label. Urban et al. \cite{urban2016deep} use a small shallow net to mimick the output of a large deep net while using layer-wised distillation with $\ell_2$ loss of the outputs of student and teacher net. Similarly, Luo et al. \cite{luo2016face} use $\ell_2$ loss to train a compressed student model from the teacher model for face recognition. Gupta et al. \cite{Gupta_2016_CVPR} use supervision transfer to distill the knowledge from a trained CNN with unlabeled data or just a few labeled data.

The limitation of the previous work in Distillation is that it is difficult to balance the importance of the knowledge from source model and the true label from the training data to train the target model. Previous studies avoid this problem by either using brutal force searching or domain knowledge. In this thesis, we proposed a novel method that can autonomously balance the importance and extend it to the semi-supervised domain adaptation.

