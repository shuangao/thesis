Convolutional Neural Networks \cite{lecun1998gradient} is the most popular and powerful method for image recognition task. In this part, we introduce the history of Convolutional Neural Networks. The detail description of the Convolutional Neural Networks layers will be included in chapter \ref{sec:cnn}.

\subsubsection{Early Work with Convolutional Neural Networks}
The first simple version of Neural Networks (NNs) trained with supervised learning was proposed in 1960s  \cite{rosenblatt1958perceptron}\cite{rosenblatt1962principles}. Networks trained by the Group Method of Data Handling (GMDH) could be the first DL systems of the Feedforward Multilayer Perceptron type \cite{ivakhnenko1965cybernetic}\cite{Schmidhuber14}. Later, there have been many applications of GMDH-style
nets \cite{farlow1984self} \cite{ikeda1976sequential} \cite{kondo2008multi} \cite{witczak2006gmdh}.

Apart from deep GMDH networks, the Neocognitron, a hierarchical, multilayered artificial neural network, was perhaps the first artificial NN to incorporate the
neurophysiological insights \cite{fukushima1980neocognitron}. Inspired by Neocognitron, Convolutional NNs (CNNs) was proposed where the rectangular receptive field of a convolutional unit with given weight vector is shifted step by step across a 2-dimensional array of input values, such as the pixels of an image (usually there are several such filters). The results of the previous unit can provide inputs to higher-level units, and so on. Because of its massive weight replication,  relatively few parameters may be necessary to describe its behavior.

In 1989, backpropagation \cite{lecun1989backpropagation}\cite{lecun1998gradient} was applied to Convolutional Neural networks with adaptive connections \cite{lecun1989backpropagation}. This combination, incorporating with Max-Pooling and speeding up on graphics cards has become an important part for many modern, competition-winning, feedforward, visual Deep Learners. Later, CNNs achieved good performance on many practical tasks such as MNIST and fingerprint recognition and was commercially used in these fields in 1990s \cite{baldi1993neural} \cite{le1990handwritten}. 

In the early 2000s, even though GPU-MPCNNs wons several official contests, many practical and commercial pattern recognition applications were dominated by non-neural machine learning methods such as Support Vector Machines (SVMs).

\subsubsection{Recent Achievements with Convlutional Neural Networks}
In 2006, CNN trained with backpropagation set a new MNIST record of 0.39\% without using unsupervised pre-training \cite{marc2006efficient}. Also in 2006, an early GPU-based CNN implementation was introduced which was up to 4 times faster than CPU-CNNs \cite{chellapilla2006high}. Since then, GPUs or graphics cards have become more and more essential for CNNs in recent years. In 2012, a GPU implemented Max-Pooling CNNs (GPU-MPCNNs) was also the first method to achieve human-competitive performance (around 0.2\%) on MNIST \cite{ciresan2012multi}.

In 2012, an ensemble of GPU-MPCNNs (called AlexNet) achieved best results (top-5 accuracy at 83\%) on the ImageNet classification benchmark (ILSVRC2012), which contains 1000 classes and 1.2 million images \cite{krizhevsky2012imagenet}. After that, excellent results have been achieved by GPU-MPCNNs in image recognition and classification. Many attempts have been made to improve the architecture of AlexNet. With the help of high performance computing systems, such as GPUs and large scale distributed cluster, some improvements have been made by either making the network deeper or increasing the size of the training data  (with extra training example and data argumentation). By reducing the size of the receptive field and stride, Zeiler and Fergus improve AlexNet by 1.7\% on top 5 accuracy \cite{zeiler2014visualizing}. By both adding extra convolutional layers between two pooling layers and reducing the receptive field size, Simonyan and Zisserman built a 19 layer very deep CNN and achieved 92.5\% top-5 accuracy \cite{simonyan2014very}. After the AlexNet-like deep CNNs won ILSVRC2012 and ILSVRC2013, Szegedy et al. built a 22-layer deep network, called GoogLeNet and won the 1st prize on ILSVRC2014 for 93.33\% top-5 accuracy, almost as good as human annotation\cite{szegedy2014going}. Different from AlexNet-like architecture, GoogLeNet shows another trend of design, utilizing many $1\times 1$ receptive field. Recently, Wu et. al present an image recognition system by aggressive data augmentation on the training data, achieving a top-5 error rate of 5.33\% on ImageNet dataset\cite{wu2015deep}. Searchers from Google successfully trained an ingredient detector system based on GoogLeNet with 220 million images harvested from Google Images and Flickr \cite{malmaud2015s}.  

Besides its impressive performance on those huge datasets, MPCNNs shows some impressive results by fine-tuning the existing models on small datasets.Zeiler et al. applied their pre-trained model on Caltech-256 with just 15 instances per class and improved the previous state-of-the-art in which about 60 instances were used, by almost 10\% \cite{zeiler2014visualizing}. Chatfield et al. used their pre-trained model on VOC2007 dataset and outperformed the previous state-of-the-art by 0.9\% \cite{Chatfield14}. Zhou et al. trained AlexNet for Scene Recognition across two datasets with identical categories and provided the state-of-the-art performance using our deep features on all the current scene benchmarks \cite{NIPS2014_Zhou}. Hoffman et al. fine-tuned the MPCNNs trained from ImageNet with one example per class, showing that it is possible to use a hybrid approach where one uses different feature representations for the various domains
and produces a combined adapted model \cite{hoffman2013one}.

To summarize, in this section, we have briefly reviewed three types of classifier for image recognition, namely Softmax classifier, SVM and CNNs. Softmax classifier is typically used as the last layer of CNNs. SVM is a more general method for the classification task. Moreover, The generalization ability of SVM classifier can reduce the bias from the training data.
