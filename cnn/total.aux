\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Learning Food Recognition Model with Deep Representation}{64}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:cnn}{{5}{64}}
\newsmartlabel{sec:cnn}{{5}{5}}
\newnamelabel{sec:cnn}{{Learning Food Recognition Model with Deep Representation}{Learning Food Recognition Model with Deep Representation}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{64}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Tuning the Deep CNNs}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Demonstration of Fine-tuning from ImageNet 1000 classes to Food-101 datasets.\relax }}{65}}
\newlabel{fig:ft-net}{{5.1}{65}}
\newsmartlabel{fig:ft-net}{{5.1}{5}}
\newnamelabel{fig:ft-net}{{Tuning the Deep CNNs}{Tuning the Deep CNNs}}
\citation{krizhevsky2012imagenet}
\citation{lecun1998gradient}
\citation{simonyan2014very}
\citation{zeiler2014visualizing}
\citation{szegedy2014going}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Layers in Deep CNN}{66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Convolutional Layer}{66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Pooling Layer}{66}}
\citation{malmaud2015s}
\citation{szegedy2014going}
\citation{boureau2010theoretical}
\citation{yang2009linear}
\citation{hinton2012improving}
\citation{nair2010rectified}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Convolution operation with $3\times 3$ kernel, stride 1 and padding 1. $\otimes $ denotes the convolutional operator.\relax }}{67}}
\newlabel{fig:cnn:conv}{{5.2}{67}}
\newsmartlabel{fig:cnn:conv}{{5.2}{5}}
\newnamelabel{fig:cnn:conv}{{Convolutional Layer}{Convolutional Layer}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Fully Connected Layer}{67}}
\citation{jarrett2009best}
\citation{srivastava2014dropout}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces $2\times 2$ pooling layer with stride 2 and padding 0.\relax }}{68}}
\newlabel{fig:cnn:pool}{{5.3}{68}}
\newsmartlabel{fig:cnn:pool}{{5.3}{5}}
\newnamelabel{fig:cnn:pool}{{Pooling Layer}{Pooling Layer}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3.1}Rectified Linear Units (ReLUs) for Activation}{68}}
\newlabel{eq:cnn:relu}{{5.3.3.1}{68}}
\newsmartlabel{eq:cnn:relu}{{5.3.3.1}{5}}
\newnamelabel{eq:cnn:relu}{{Rectified Linear Units (ReLUs) for Activation}{Rectified Linear Units (ReLUs) for Activation}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3.2}DropOut}{68}}
\citation{jia2014caffe}
\citation{srivastava2014dropout}
\citation{linNiN}
\citation{szegedy2014going}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { Standard Neural Net }}}{69}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {After Dropout}}}{69}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Experiment Settings}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Models}{69}}
\citation{kawano14c}
\citation{bossard2014food}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Inception Module. $n\times n$ stands for size $n$ receptive field, $n\times n\_reduce$ stands for the $1\times 1$ convolutional layer before the $n\times n$ convolution layer and $pool\_proj$ is another $1\times 1$ convolutional layer after the MAX pooling layer. The output layer concatenates all its input layers.\relax }}{70}}
\newlabel{incept}{{5.4}{70}}
\newsmartlabel{incept}{{5.4}{5}}
\newnamelabel{incept}{{Models}{Models}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Food Datasets}{70}}
\citation{wu2015deep}
\citation{krizhevsky2012imagenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Data Argumentation}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Crop area from original image\relax }}{72}}
\newlabel{fig:cnn:crop}{{5.5}{72}}
\newsmartlabel{fig:cnn:crop}{{5.5}{5}}
\newnamelabel{fig:cnn:crop}{{Data Argumentation}{Data Argumentation}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Original image}}}{72}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Center}}}{72}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Center mirror}}}{72}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Up-left}}}{72}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {Up-left mirror}}}{72}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {Up-right}}}{72}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {Up-right mirror}}}{72}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {Bottom-left}}}{72}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {Bottom-left mirror}}}{72}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(j)}{\ignorespaces {Bottom-right}}}{72}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(k)}{\ignorespaces {Bottom-right mirror}}}{72}}
\citation{agrawal2014analyzing}
\citation{glorot2010understanding}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Experimental configuration for GoogLeNet\relax }}{73}}
\newlabel{tab:config}{{5.1}{73}}
\newsmartlabel{tab:config}{{5.1}{5}}
\newnamelabel{tab:config}{{Data Argumentation}{Data Argumentation}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Discussion}{73}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Different data argumentation methods\relax }}{74}}
\newlabel{fig:cnn:argu}{{5.6}{74}}
\newsmartlabel{fig:cnn:argu}{{5.6}{5}}
\newnamelabel{fig:cnn:argu}{{Data Argumentation}{Data Argumentation}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Original image}}}{74}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Red casting}}}{74}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Green casting}}}{74}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Blue casting}}}{74}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {RGB casting}}}{74}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {Vignette}}}{74}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {More vignette}}}{74}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {Horizontal stretch}}}{74}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {More horizontal stretch}}}{74}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(j)}{\ignorespaces {Vertical stretch}}}{74}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(k)}{\ignorespaces {More vertical stretch}}}{74}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(l)}{\ignorespaces {Rotation}}}{74}}
\citation{Kawano:2014}
\citation{bossard2014food}
\citation{singh2012unsupervised}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Pre-training and Fine-tuning}{75}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Top-5 Accuracy in percent on fine-tuned, ft-last and scratch model for two architectures\relax }}{75}}
\newlabel{tab:ft}{{5.2}{75}}
\newsmartlabel{tab:ft}{{5.2}{5}}
\newnamelabel{tab:ft}{{Pre-training and Fine-tuning}{Pre-training and Fine-tuning}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Accuracy compared to other methods on Food-256 dataset in percent\relax }}{75}}
\newlabel{tab:256}{{5.3}{75}}
\newsmartlabel{tab:256}{{5.3}{5}}
\newnamelabel{tab:256}{{Pre-training and Fine-tuning}{Pre-training and Fine-tuning}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Top-1 accuracy compared to other methods on Food-101 dataset in percent\relax }}{75}}
\newlabel{tab:101}{{5.4}{75}}
\newsmartlabel{tab:101}{{5.4}{5}}
\newnamelabel{tab:101}{{Pre-training and Fine-tuning}{Pre-training and Fine-tuning}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Visualization of some feature maps of different GoogLeNet models in different layers for the same input image. 64 feature maps of each layer are shown. Conv1 is the first convolutional layer and Inception\_5b is the last convolutional layer. \relax }}{76}}
\newlabel{fig:sashimi}{{5.7}{76}}
\newsmartlabel{fig:sashimi}{{5.7}{5}}
\newnamelabel{fig:sashimi}{{Pre-training and Fine-tuning}{Pre-training and Fine-tuning}}
\newlabel{relu}{{5.3}{76}}
\newsmartlabel{relu}{{5.3}{5}}
\citation{NIPS2014_Zhou}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Learning across the datasets}{77}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Cosine similarity of the layers in Inception modules between fine-tuned models and pre-trained model for GoogLeNet\relax }}{78}}
\newlabel{tab:cosg}{{5.5}{78}}
\newsmartlabel{tab:cosg}{{5.5}{5}}
\newnamelabel{tab:cosg}{{Pre-training and Fine-tuning}{Pre-training and Fine-tuning}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces Cosine similarity of the layers between fine-tuned models and pre-trained model for AlexNet\relax }}{78}}
\newlabel{tab:cosa}{{5.6}{78}}
\newsmartlabel{tab:cosa}{{5.6}{5}}
\newnamelabel{tab:cosa}{{Pre-training and Fine-tuning}{Pre-training and Fine-tuning}}
\@writefile{lot}{\contentsline {table}{\numberline {5.7}{\ignorespaces Sparsity of the output for each unit in GoogLeNet inception module for training data from Food101 in percent\relax }}{79}}
\newlabel{tab:sparse}{{5.7}{79}}
\newsmartlabel{tab:sparse}{{5.7}{5}}
\newnamelabel{tab:sparse}{{Pre-training and Fine-tuning}{Pre-training and Fine-tuning}}
\@writefile{lot}{\contentsline {table}{\numberline {5.8}{\ignorespaces Top5 Accuracy for transferring from Food101 to subset of Food256 in percent\relax }}{79}}
\newlabel{tab:cross}{{5.8}{79}}
\newsmartlabel{tab:cross}{{5.8}{5}}
\newnamelabel{tab:cross}{{Learning across the datasets}{Learning across the datasets}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Summary}{80}}
\@setckpt{cnn/total}{
\setcounter{page}{81}
\setcounter{equation}{3}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{2}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{5}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{7}
\setcounter{table}{8}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{parentequation}{0}
\setcounter{@smartlistlen}{2}
\setcounter{less@smartlist}{0}
\setcounter{@currsmartlistplace}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{12}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lips@count}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{3}
\setcounter{ALC@unique}{32}
\setcounter{ALC@line}{15}
\setcounter{ALC@rem}{15}
\setcounter{ALC@depth}{0}
\setcounter{defi}{0}
\setcounter{theorem}{0}
\setcounter{myappendices}{0}
\setcounter{appdepth}{1}
}
