\relax 
\citation{rosenblatt1958perceptron}
\citation{rosenblatt1962principles}
\citation{ivakhnenko1965cybernetic}
\citation{Schmidhuber14}
\citation{farlow1984self}
\citation{ikeda1976sequential}
\citation{kondo2008multi}
\citation{witczak2006gmdh}
\citation{fukushima1980neocognitron}
\citation{lecun1989backpropagation}
\citation{baldi1993neural}
\citation{le1990handwritten}
\citation{marc2006efficient}
\citation{chellapilla2006high}
\citation{ciresan2012multi}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Convolutional NN}{7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Background: brief history and recent achievments}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Early work with Convlutional Neural Networks}{7}}
\citation{krizhevsky2012imagenet}
\citation{zeiler2014visualizing}
\citation{simonyan2014very}
\citation{szegedy2014going}
\citation{wu2015deep}
\citation{malmaud2015s}
\citation{zeiler2014visualizing}
\citation{Chatfield14}
\citation{NIPS2014_Zhou}
\citation{hoffman2013one}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Recent achievements with Convlutional Neural Networks}{8}}
\citation{krizhevsky2012imagenet}
\citation{lecun1998gradient}
\citation{simonyan2014very}
\citation{zeiler2014visualizing}
\citation{szegedy2014going}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}CNN layers:conv/pool/norm etc}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Convolutional Layer}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Pooling Layer}{9}}
\citation{malmaud2015s}
\citation{szegedy2014going}
\citation{boureau2010theoretical}
\citation{yang2009linear}
\citation{hinton2012improving}
\citation{nair2010rectified}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Convolution operation with $3\times 3$ kernel, stride 1 and padding 1. $\otimes $ denotes the convolutional operator.\relax }}{10}}
\newlabel{fig:cnn:conv}{{2.1}{10}}
\newsmartlabel{fig:cnn:conv}{{2.1}{2}}
\newnamelabel{fig:cnn:conv}{{Convolutional Layer}{Convolutional Layer}}
\citation{jarrett2009best}
\citation{srivastava2014dropout}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces $2\times 2$ pooling layer with stride 2 and padding 0.\relax }}{11}}
\newlabel{fig:cnn:pool}{{2.2}{11}}
\newsmartlabel{fig:cnn:pool}{{2.2}{2}}
\newnamelabel{fig:cnn:pool}{{Pooling Layer}{Pooling Layer}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Fully Connected Layer}{11}}
\@writefile{toc}{\contentsline {subsubsection}{Rectified Linear Units (ReLUs) for Activation}{11}}
\newlabel{eq:cnn:relu}{{2.2.3}{11}}
\newsmartlabel{eq:cnn:relu}{{2.2.3}{2}}
\newnamelabel{eq:cnn:relu}{{Rectified Linear Units (ReLUs) for Activation}{Rectified Linear Units (ReLUs) for Activation}}
\citation{jia2014caffe}
\citation{srivastava2014dropout}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { Standard Neural Net }}}{12}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {After Dropout}}}{12}}
\@writefile{toc}{\contentsline {subsubsection}{DropOut}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Datasets}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Models}{12}}
\citation{linNiN}
\citation{szegedy2014going}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Inception Module. $n\times n$ stands for size $n$ receptive field, $n\times n\_reduce$ stands for the $1\times 1$ convolutional layer before the $n\times n$ convolution layer and $pool\_proj$ is another $1\times 1$ convolutional layer after the MAX pooling layer. The output layer concatenates all its input layers.\relax }}{13}}
\newlabel{incept}{{2.3}{13}}
\newsmartlabel{incept}{{2.3}{2}}
\newnamelabel{incept}{{Models}{Models}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Food Datasets}{13}}
\citation{kawano14c}
\citation{bossard2014food}
\citation{wu2015deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Data Argumentation}{14}}
\citation{krizhevsky2012imagenet}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Crop area from original image\relax }}{15}}
\newlabel{fig:cnn:crop}{{2.4}{15}}
\newsmartlabel{fig:cnn:crop}{{2.4}{2}}
\newnamelabel{fig:cnn:crop}{{Data Argumentation}{Data Argumentation}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Original image}}}{15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Center}}}{15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Center mirror}}}{15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Up-left}}}{15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {Up-left mirror}}}{15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {Up-right}}}{15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {Up-right mirror}}}{15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {Bottom-left}}}{15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {Bottom-left mirror}}}{15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(j)}{\ignorespaces {Bottom-right}}}{15}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(k)}{\ignorespaces {Bottom-right mirror}}}{15}}
\citation{agrawal2014analyzing}
\citation{glorot2010understanding}
\citation{Kawano:2014}
\citation{bossard2014food}
\citation{singh2012unsupervised}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Experimental Discuss}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Pre-training and Fine-tuning}{16}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Top-5 Accuracy in percent on fine-tuned, ft-last and scratch model for two architectures\relax }}{16}}
\newlabel{tab:ft}{{2.1}{16}}
\newsmartlabel{tab:ft}{{2.1}{2}}
\newnamelabel{tab:ft}{{Pre-training and Fine-tuning}{Pre-training and Fine-tuning}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Different data argumentation methods\relax }}{17}}
\newlabel{fig:cnn:argu}{{2.5}{17}}
\newsmartlabel{fig:cnn:argu}{{2.5}{2}}
\newnamelabel{fig:cnn:argu}{{Data Argumentation}{Data Argumentation}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Original image}}}{17}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Red casting}}}{17}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Green casting}}}{17}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Blue casting}}}{17}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {RGB casting}}}{17}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {Vignette}}}{17}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {More vignette}}}{17}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {Horizontal stretch}}}{17}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {More horizontal stretch}}}{17}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(j)}{\ignorespaces {Vertical stretch}}}{17}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(k)}{\ignorespaces {More vertical stretch}}}{17}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(l)}{\ignorespaces {Rotation}}}{17}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Accuracy compared to other method on Food-256 dataset in percent\relax }}{17}}
\newlabel{tab:256}{{2.2}{17}}
\newsmartlabel{tab:256}{{2.2}{2}}
\newnamelabel{tab:256}{{Pre-training and Fine-tuning}{Pre-training and Fine-tuning}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Top-1 accuracy compared to other methods on Food-101 dataset in percent\relax }}{18}}
\newlabel{tab:101}{{2.3}{18}}
\newsmartlabel{tab:101}{{2.3}{2}}
\newnamelabel{tab:101}{{Pre-training and Fine-tuning}{Pre-training and Fine-tuning}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Visualization of some feature maps of different GoogLeNet models in different layers for the same input image. 64 feature maps of each layer are shown. Conv1 is the first convolutional layer and Inception\_5b is the last convolutional layer. \relax }}{18}}
\newlabel{fig:sashimi}{{2.6}{18}}
\newsmartlabel{fig:sashimi}{{2.6}{2}}
\newnamelabel{fig:sashimi}{{Pre-training and Fine-tuning}{Pre-training and Fine-tuning}}
\citation{NIPS2014_Zhou}
\newlabel{relu}{{2.3}{19}}
\newsmartlabel{relu}{{2.3}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Learning across the datasets}{19}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Cosine similarity of the layers in inception modules between fine-tuned models and pre-trained model for GoogLeNet\relax }}{20}}
\newlabel{tab:cosg}{{2.4}{20}}
\newsmartlabel{tab:cosg}{{2.4}{2}}
\newnamelabel{tab:cosg}{{Pre-training and Fine-tuning}{Pre-training and Fine-tuning}}
\@writefile{lot}{\contentsline {table}{\numberline {2.5}{\ignorespaces Cosine similarity of the layers between fine-tuned models and pre-trained model for AlexNet\relax }}{20}}
\newlabel{tab:cosa}{{2.5}{20}}
\newsmartlabel{tab:cosa}{{2.5}{2}}
\newnamelabel{tab:cosa}{{Pre-training and Fine-tuning}{Pre-training and Fine-tuning}}
\@writefile{lot}{\contentsline {table}{\numberline {2.6}{\ignorespaces Sparsity of the output for each unit in GoogLeNet inception module for training data from Food101 in percent\relax }}{21}}
\newlabel{tab:sparse}{{2.6}{21}}
\newsmartlabel{tab:sparse}{{2.6}{2}}
\newnamelabel{tab:sparse}{{Pre-training and Fine-tuning}{Pre-training and Fine-tuning}}
\@writefile{lot}{\contentsline {table}{\numberline {2.7}{\ignorespaces Top5 Accuracy for transferring from Food101 to subset of Food256 in percent\relax }}{22}}
\newlabel{tab:cross}{{2.7}{22}}
\newsmartlabel{tab:cross}{{2.7}{2}}
\newnamelabel{tab:cross}{{Learning across the datasets}{Learning across the datasets}}
\@setckpt{cnn/total}{
\setcounter{page}{23}
\setcounter{equation}{3}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{2}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{6}
\setcounter{table}{7}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{parentequation}{0}
\setcounter{@smartlistlen}{2}
\setcounter{less@smartlist}{0}
\setcounter{@currsmartlistplace}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{12}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lips@count}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{theorem}{0}
\setcounter{myappendices}{0}
\setcounter{appdepth}{1}
}
