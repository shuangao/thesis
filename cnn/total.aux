\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Learning Food Recognition Model with Deep Representation}{64}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:cnn}{{5}{64}{Learning Food Recognition Model with Deep Representation}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{64}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Tuning the Deep CNNs}{64}{section.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Demonstration of Fine-tuning from ImageNet 1000 classes to Food-101 datasets.\relax }}{65}{figure.caption.35}}
\newlabel{fig:ft-net}{{5.1}{65}{Demonstration of Fine-tuning from ImageNet 1000 classes to Food-101 datasets.\relax }{figure.caption.35}{}}
\citation{krizhevsky2012imagenet}
\citation{lecun1998gradient}
\citation{simonyan2014very}
\citation{zeiler2014visualizing}
\citation{szegedy2014going}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Layers in Deep CNN}{66}{section.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Convolutional Layer}{66}{subsection.5.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Pooling Layer}{66}{subsection.5.3.2}}
\citation{malmaud2015s}
\citation{szegedy2014going}
\citation{boureau2010theoretical}
\citation{yang2009linear}
\citation{hinton2012improving}
\citation{nair2010rectified}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Convolution operation with $3\times 3$ kernel, stride 1 and padding 1. $\otimes $ denotes the convolutional operator.\relax }}{67}{figure.caption.36}}
\newlabel{fig:cnn:conv}{{5.2}{67}{Convolution operation with $3\times 3$ kernel, stride 1 and padding 1. $\otimes $ denotes the convolutional operator.\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Fully Connected Layer}{67}{subsection.5.3.3}}
\citation{jarrett2009best}
\citation{srivastava2014dropout}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces $2\times 2$ pooling layer with stride 2 and padding 0.\relax }}{68}{figure.caption.37}}
\newlabel{fig:cnn:pool}{{5.3}{68}{$2\times 2$ pooling layer with stride 2 and padding 0.\relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3.1}Rectified Linear Units (ReLUs) for Activation}{68}{subsubsection.5.3.3.1}}
\newlabel{eq:cnn:relu}{{5.3.3.1}{68}{Rectified Linear Units (ReLUs) for Activation}{equation.5.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3.2}DropOut}{68}{subsubsection.5.3.3.2}}
\citation{jia2014caffe}
\citation{srivastava2014dropout}
\citation{linNiN}
\citation{szegedy2014going}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Dropout Layers. Adopted from Standford CS231n Convolutional Neural Networks for Visual Recognition\relax }}{69}{figure.caption.38}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { Standard Neural Net }}}{69}{subfigure.4.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {After Dropout}}}{69}{subfigure.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Experiment Settings}{69}{section.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Models}{69}{subsection.5.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Inception Module. $n\times n$ stands for size $n$ receptive field, $n\times n\_reduce$ stands for the $1\times 1$ convolutional layer before the $n\times n$ convolution layer and $pool\_proj$ is another $1\times 1$ convolutional layer after the MAX pooling layer. The output layer concatenates all its input layers.\relax }}{70}{figure.caption.39}}
\newlabel{incept}{{5.5}{70}{Inception Module. $n\times n$ stands for size $n$ receptive field, $n\times n\_reduce$ stands for the $1\times 1$ convolutional layer before the $n\times n$ convolution layer and $pool\_proj$ is another $1\times 1$ convolutional layer after the MAX pooling layer. The output layer concatenates all its input layers.\relax }{figure.caption.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Food Datasets}{70}{subsection.5.4.2}}
\citation{kawano14c}
\citation{bossard2014food}
\citation{wu2015deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Data Augmentation}{71}{subsection.5.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Crop area from original image\relax }}{72}{figure.caption.40}}
\newlabel{fig:cnn:crop}{{5.6}{72}{Crop area from original image\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Original image}}}{72}{subfigure.6.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Center}}}{72}{subfigure.6.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Center mirror}}}{72}{subfigure.6.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Up-left}}}{72}{subfigure.6.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {Up-left mirror}}}{72}{subfigure.6.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {Up-right}}}{72}{subfigure.6.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {Up-right mirror}}}{72}{subfigure.6.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {Bottom-left}}}{72}{subfigure.6.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {Bottom-left mirror}}}{72}{subfigure.6.9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(j)}{\ignorespaces {Bottom-right}}}{72}{subfigure.6.10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(k)}{\ignorespaces {Bottom-right mirror}}}{72}{subfigure.6.11}}
\citation{krizhevsky2012imagenet}
\citation{agrawal2014analyzing}
\citation{glorot2010understanding}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Experimental configuration for GoogLeNet\relax }}{73}{table.caption.42}}
\newlabel{tab:config}{{5.1}{73}{Experimental configuration for GoogLeNet\relax }{table.caption.42}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Discussion}{73}{section.5.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Different data augmentation methods\relax }}{74}{figure.caption.41}}
\newlabel{fig:cnn:argu}{{5.7}{74}{Different data augmentation methods\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Original image}}}{74}{subfigure.7.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Red casting}}}{74}{subfigure.7.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Green casting}}}{74}{subfigure.7.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Blue casting}}}{74}{subfigure.7.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {RGB casting}}}{74}{subfigure.7.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {Vignette}}}{74}{subfigure.7.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {More vignette}}}{74}{subfigure.7.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {Horizontal stretch}}}{74}{subfigure.7.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {More horizontal stretch}}}{74}{subfigure.7.9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(j)}{\ignorespaces {Vertical stretch}}}{74}{subfigure.7.10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(k)}{\ignorespaces {More vertical stretch}}}{74}{subfigure.7.11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(l)}{\ignorespaces {Rotation}}}{74}{subfigure.7.12}}
\citation{Kawano:2014}
\citation{bossard2014food}
\citation{singh2012unsupervised}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Pre-training and Fine-tuning}{75}{subsection.5.5.1}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Top-5 Accuracy in percent on fine-tuned, ft-last and scratch model for two architectures\relax }}{75}{table.caption.43}}
\newlabel{tab:ft}{{5.2}{75}{Top-5 Accuracy in percent on fine-tuned, ft-last and scratch model for two architectures\relax }{table.caption.43}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Accuracy compared to other methods on Food-256 dataset in percent\relax }}{75}{table.caption.44}}
\newlabel{tab:256}{{5.3}{75}{Accuracy compared to other methods on Food-256 dataset in percent\relax }{table.caption.44}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Top-1 accuracy compared to other methods on Food-101 dataset in percent\relax }}{75}{table.caption.45}}
\newlabel{tab:101}{{5.4}{75}{Top-1 accuracy compared to other methods on Food-101 dataset in percent\relax }{table.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Visualization of some feature maps of different GoogLeNet models in different layers for the same input image. 64 feature maps of each layer are shown. Conv1 is the first convolutional layer and Inception\_5b is the last convolutional layer. \relax }}{76}{figure.caption.46}}
\newlabel{fig:sashimi}{{5.8}{76}{Visualization of some feature maps of different GoogLeNet models in different layers for the same input image. 64 feature maps of each layer are shown. Conv1 is the first convolutional layer and Inception\_5b is the last convolutional layer. \relax }{figure.caption.46}{}}
\newlabel{relu}{{5.3}{76}{Pre-training and Fine-tuning}{equation.5.5.3}{}}
\citation{NIPS2014_Zhou}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Learning across the datasets}{77}{subsection.5.5.2}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Cosine similarity of the layers in Inception modules between fine-tuned models and pre-trained model for GoogLeNet\relax }}{78}{table.caption.47}}
\newlabel{tab:cosg}{{5.5}{78}{Cosine similarity of the layers in Inception modules between fine-tuned models and pre-trained model for GoogLeNet\relax }{table.caption.47}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces Cosine similarity of the layers between fine-tuned models and pre-trained model for AlexNet\relax }}{78}{table.caption.48}}
\newlabel{tab:cosa}{{5.6}{78}{Cosine similarity of the layers between fine-tuned models and pre-trained model for AlexNet\relax }{table.caption.48}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.7}{\ignorespaces Sparsity of the output for each unit in GoogLeNet inception module for training data from Food101 in percent\relax }}{79}{table.caption.49}}
\newlabel{tab:sparse}{{5.7}{79}{Sparsity of the output for each unit in GoogLeNet inception module for training data from Food101 in percent\relax }{table.caption.49}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.8}{\ignorespaces Top5 Accuracy for transferring from Food101 to subset of Food256 in percent\relax }}{79}{table.caption.50}}
\newlabel{tab:cross}{{5.8}{79}{Top5 Accuracy for transferring from Food101 to subset of Food256 in percent\relax }{table.caption.50}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Summary}{80}{section.5.6}}
\@setckpt{cnn/total}{
\setcounter{page}{81}
\setcounter{equation}{3}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{2}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{5}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{8}
\setcounter{table}{8}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{parentequation}{0}
\setcounter{@smartlistlen}{2}
\setcounter{less@smartlist}{0}
\setcounter{@currsmartlistplace}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{12}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lips@count}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{3}
\setcounter{ALC@unique}{32}
\setcounter{ALC@line}{15}
\setcounter{ALC@rem}{15}
\setcounter{ALC@depth}{0}
\setcounter{defi}{0}
\setcounter{Item}{5}
\setcounter{Hfootnote}{5}
\setcounter{bookmark@seq@number}{86}
\setcounter{theorem}{0}
\setcounter{myappendices}{0}
\setcounter{appdepth}{1}
\setcounter{section@level}{1}
}
