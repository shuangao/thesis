\chapter{Effective Multiclass Transfer For Hypothesis Transfer Learning}\label{sec:pakdd}
\section{Introduction}
Domain adaptation for image recognition tries to exploit the knowledge from a source domain with plentiful data to help learn a classifier for the target domain with a different distribution and little labeled training data. In domain adaptation, the source and target domains share the same label but their data are drawn from different distributions.

Previous research \cite{ben2010theory,ben2007analysis} shows that without carefully measuring the distribution similarity between the source and target data, the source knowledge could not be exploited effectively or even hurt the learning process (called  \textit{negative transfer})\cite{pan2010survey}. 
However, as we are not able to access the source data in the Hypothesis Transfer Learning (HTL) \cite{kuzborskij2013stability} setting, how to effectively and safely exploit the knowledge from the source model could be an important issue in HTL, especially when target data is relatively small (Effectiveness issue). Moreover, the source models from different domains can be trained with different kinds of classifiers. For example, most models trained from ImageNet are deep convolutional neural networks while some models of the VOC recognition task could be SVMs or ensemble models. Therefore, a practical HTL algorithm should be compatible with different types of source classifiers (Compatibility issue). Previous work is limited to either leveraging the knowledge from a certain type of source classifiers \cite{tommasi2014learning,fei2006one} or low transfer efficiency in a small training set\cite{jie2011multiclass}. To the best of our knowledge, none of the previous work in HTL is able to solve these two issues at the same time.

In this chapter, we propose our method, called \textbf{Effective Multiclass Transfer Learning} (EMTLe), that can solve these two issues simultaneously. We perform comprehensive experiments on 4 real-world datasets from two benchmark datasets (3 from Office and 1 from Caltech256). We show that EMTLe can effectively transfer the knowledge with different types of source models and outperforms the baseline methods under the HTL setting. 

This work has been accepted by \textit{Pacific-Asia Conference on Knowledge Discovery and Data Mining, 2017}.

\section{Using the Source Knowledge as the Auxiliary Bias}\label{sec:prob}
\input{pakdd/statement2.tex}

\section{Bi-level Optimization for Transfer Parameter Estimation}\label{sec:smitle}
\input{pakdd/BO.tex}
%\input{smitle.tex}

\section{Experiments}\label{sec:exp}
\input{pakdd/gama_exp.tex}

\section{Summary}
\input{pakdd/gama_con.tex}