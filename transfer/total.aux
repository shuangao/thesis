\relax 
\citation{pan2010survey}
\citation{tommasi2014learning}
\citation{lim2012transfer}
\citation{LongICML15}
\citation{yang2007cross}
\citation{aytar2011tabula}
\citation{tommasi2014learning}
\citation{kuzborskij2013n}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Learning Single Source Categories}{26}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Issues in Transfer Learning}{26}}
\citation{davis2009deep}
\citation{wang2014active}
\citation{zhou2014multi}
\citation{kuzborskij2013n}
\citation{tommasi2010safety}
\citation{cawley2006leave}
\citation{suykens1999least}
\citation{yang2007adapting}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Related Work}{28}}
\newlabel{sec:single:rl}{{3.2}{28}}
\newsmartlabel{sec:single:rl}{{3.2}{3}}
\newnamelabel{sec:single:rl}{{Related Work}{Related Work}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}LS-SVM Classifier}{28}}
\newlabel{sec:single:lssvm}{{3.2.1}{28}}
\newsmartlabel{sec:single:lssvm}{{3.2.1}{3}}
\newnamelabel{sec:single:lssvm}{{LS-SVM Classifier}{LS-SVM Classifier}}
\newlabel{eq:gama:lssvm}{{3.1}{28}}
\newsmartlabel{eq:gama:lssvm}{{3.1}{3}}
\newlabel{sq:gama:lsprime}{{3.2}{28}}
\newsmartlabel{sq:gama:lsprime}{{3.2}{3}}
\newlabel{eq:single:orgmatrix}{{3.3}{28}}
\newsmartlabel{eq:single:orgmatrix}{{3.3}{3}}
\newlabel{eq:gama:psi}{{3.4}{28}}
\newsmartlabel{eq:gama:psi}{{3.4}{3}}
\citation{aytar2011tabula}
\citation{aytar2011tabula}
\citation{aytar2011tabula}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Projecting $w$ to $w'$ in PMT-SVM (adapted from \cite  {aytar2011tabula}).\relax }}{29}}
\newlabel{fig:gama:pmt}{{3.1}{29}}
\newsmartlabel{fig:gama:pmt}{{3.1}{3}}
\newnamelabel{fig:gama:pmt}{{ASVM \& PMT-SVM}{ASVM \& PMT-SVM}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}ASVM \& PMT-SVM}{29}}
\newlabel{eq:gama:asvm}{{3.6}{29}}
\newsmartlabel{eq:gama:asvm}{{3.6}{3}}
\citation{tommasi2014learning}
\citation{cawley2006leave}
\newlabel{eq:gama:pmt}{{3.7}{30}}
\newsmartlabel{eq:gama:pmt}{{3.7}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Multi-KT}{30}}
\newlabel{eq:gama:multi}{{3.8}{30}}
\newsmartlabel{eq:gama:multi}{{3.8}{3}}
\citation{kuzborskij2013n}
\citation{crammer2002algorithmic}
\citation{BoydCO}
\newlabel{eq:gama:loo}{{3.10}{31}}
\newsmartlabel{eq:gama:loo}{{3.10}{3}}
\newlabel{eq:gama:multib}{{3.11}{31}}
\newsmartlabel{eq:gama:multib}{{3.11}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}MULTIpLE}{31}}
\newlabel{eq:gama:multiple}{{3.12}{31}}
\newsmartlabel{eq:gama:multiple}{{3.12}{3}}
\newlabel{eq:gama:multib}{{3.13}{31}}
\newsmartlabel{eq:gama:multib}{{3.13}{3}}
\citation{kifer2004detecting}
\citation{ben2010theory}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Linear Combination Strategy}{32}}
\newlabel{sec:single:comb}{{3.3}{32}}
\newsmartlabel{sec:single:comb}{{3.3}{3}}
\newnamelabel{sec:single:comb}{{Linear Combination Strategy}{Linear Combination Strategy}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}$\mathcal  {D}$-relationship between the hypothesis and the distribution}{32}}
\newlabel{single:hdivergence}{{1}{32}}
\newsmartlabel{single:hdivergence}{{1}{3}}
\newnamelabel{single:hdivergence}{{$\mathcal  {D}$-relationship between the hypothesis and the distribution}{$\mathcal  {D}$-relationship between the hypothesis and the distribution}}
\citation{ben2010theory}
\newlabel{single:drelation}{{2}{33}}
\newsmartlabel{single:drelation}{{2}{3}}
\newnamelabel{single:drelation}{{$\mathcal  {D}$-relationship between the hypothesis and the distribution}{$\mathcal  {D}$-relationship between the hypothesis and the distribution}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Leverage the Source Knowledge with Data Arguementation}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A graphical representation of linear argumentation. The score from the source model can be considered as an auxiliary feature and the transfer parameter controls the value of the auxiliary feature. For linear classifiers, it can be considered as a linear combination of the decision from the source model and target data (see \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 3.15\hbox {}\unskip \@@italiccorr )}}).\relax }}{34}}
\newlabel{fig:single:arg}{{3.2}{34}}
\newsmartlabel{fig:single:arg}{{3.2}{3}}
\newnamelabel{fig:single:arg}{{Leverage the Source Knowledge with Data Arguementation}{Leverage the Source Knowledge with Data Arguementation}}
\citation{kuzborskij2013n}
\citation{tommasi2014learning}
\newlabel{eq:single:linear}{{3.15}{35}}
\newsmartlabel{eq:single:linear}{{3.15}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Suppose the original data is one dimensional and we use the blue line to denote the optimal decision surfce of a linear model (the upper figure). By adding an related auxiliary feature, we can improve the performacne of the classifier (the bottom-left figure) while unrelated one can decrease the performance and lead to negative transfer (the bottom-right figure). \relax }}{36}}
\newlabel{fig:single:dataarg}{{3.3}{36}}
\newsmartlabel{fig:single:dataarg}{{3.3}{3}}
\newnamelabel{fig:single:dataarg}{{Leverage the Source Knowledge with Data Arguementation}{Leverage the Source Knowledge with Data Arguementation}}
\newlabel{eq:single:reg}{{3.16}{36}}
\newsmartlabel{eq:single:reg}{{3.16}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces \texthl  {Notations used in this chapter}\relax }}{37}}
\newlabel{tab:single:notation}{{3.1}{37}}
\newsmartlabel{tab:single:notation}{{3.1}{3}}
\newnamelabel{tab:single:notation}{{Leverage the Source Knowledge with Data Arguementation}{Leverage the Source Knowledge with Data Arguementation}}
\newlabel{eq:single:formreg}{{3.17}{37}}
\newsmartlabel{eq:single:formreg}{{3.17}{3}}
\newlabel{eq:single:lssvm-deriv}{{3.19}{38}}
\newsmartlabel{eq:single:lssvm-deriv}{{3.19}{3}}
\newnamelabel{eq:single:lssvm-deriv}{{Leverage the Source Knowledge with Data Arguementation}{Leverage the Source Knowledge with Data Arguementation}}
\newlabel{eq:single:matrixsolve}{{3.20}{38}}
\newsmartlabel{eq:single:matrixsolve}{{3.20}{3}}
\newlabel{eq:single:unionreg}{{3.21}{38}}
\newsmartlabel{eq:single:unionreg}{{3.21}{3}}
\newlabel{eq:single:multisolu}{{3.22}{38}}
\newsmartlabel{eq:single:multisolu}{{3.22}{3}}
\newlabel{eq:single:labelmatrix}{{3.23}{38}}
\newsmartlabel{eq:single:labelmatrix}{{3.23}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Cross-Validation Error for LS-SVM}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Closed Form Cross-validation Estimation for LS-SVM}{39}}
\citation{cawley2006leave}
\citation{vapnik2000bounds}
\citation{kuzborskij2013stability}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces An illustration of 5-fold cross-validation\relax }}{40}}
\newlabel{th:single:cv}{{3.4.1}{40}}
\newsmartlabel{th:single:cv}{{3.4.1}{3}}
\newnamelabel{th:single:cv}{{Closed Form Cross-validation Estimation for LS-SVM}{Closed Form Cross-validation Estimation for LS-SVM}}
\newlabel{eq:single:kernel}{{3.24}{40}}
\newsmartlabel{eq:single:kernel}{{3.24}{3}}
\newlabel{eq:single:nout}{{3.4.1}{40}}
\newsmartlabel{eq:single:nout}{{3.4.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Leave-one-out Cross-validation for estimation}{41}}
\newlabel{eq:single:looesti}{{3.25}{41}}
\newsmartlabel{eq:single:looesti}{{3.25}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Multi-class Loss with LOO error}{41}}
\citation{crammer2002learnability}
\citation{crammer2002algorithmic}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Loss function comparision for multi-class hinge loss $\epsilon _{multi}$ and classical zero-one loss $\epsilon _{H}$\relax }}{42}}
\newlabel{fig:single:losscmp}{{3.5}{42}}
\newsmartlabel{fig:single:losscmp}{{3.5}{3}}
\newnamelabel{fig:single:losscmp}{{Multi-class Loss with LOO error}{Multi-class Loss with LOO error}}
\newlabel{eq:single:discreteloss}{{3.27}{42}}
\newsmartlabel{eq:single:discreteloss}{{3.27}{3}}
\newlabel{eq:single:train_loss}{{3.28}{42}}
\newsmartlabel{eq:single:train_loss}{{3.28}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Illustration of the margin bound for a single example in 3 scenarios. The cirles denote different confidences and the correct label is ploted in dark grey. The height of each lable is the confidence score. In the left figure,$\epsilon (x)=0$. In the middle figure, even though the confidence of the correct label is the largest, it fails to be larger by 1 than the confidence of the runner-up and have a small loss. In the right figure, the confidence of the correct label is not the largest one and have a very large loss.\relax }}{43}}
\newlabel{fig:single:multi-loss}{{3.6}{43}}
\newsmartlabel{fig:single:multi-loss}{{3.6}{3}}
\newnamelabel{fig:single:multi-loss}{{Multi-class Loss with LOO error}{Multi-class Loss with LOO error}}
\citation{tommasi2010safety}
\citation{kuzborskij2013n}
\citation{kuzborskij2013stability}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Transfer Parameter Optimization}{44}}
\newlabel{eq:single:opti}{{3.35}{44}}
\newsmartlabel{eq:single:opti}{{3.35}{3}}
\citation{boyd2004convex}
\newlabel{th:single:converg}{{3.6.1}{45}}
\newsmartlabel{th:single:converg}{{3.6.1}{3}}
\newnamelabel{th:single:converg}{{Transfer Parameter Optimization}{Transfer Parameter Optimization}}
\newlabel{eq:single:dual}{{3.38}{45}}
\newsmartlabel{eq:single:dual}{{3.38}{3}}
\citation{lecun1998gradient}
\newlabel{alg:1}{{\caption@xref {alg:1}{ on input line 1}}{46}}
\newsmartlabel{alg:1}{{\caption@xref {alg:1}{ on input line 1}}{3}}
\newnamelabel{alg:1}{{Transfer Parameter Optimization}{Transfer Parameter Optimization}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces SMITLe\relax }}{46}}
\newlabel{alg:1}{{1}{46}}
\newsmartlabel{alg:1}{{1}{3}}
\newnamelabel{alg:1}{{Transfer Parameter Optimization}{Transfer Parameter Optimization}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Experiment}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Dataset}{46}}
\@writefile{toc}{\contentsline {subsubsection}{MNIST}{46}}
\citation{hull1994database}
\newlabel{fig:single:MNIST}{{\caption@xref {fig:single:MNIST}{ on input line 15}}{47}}
\newsmartlabel{fig:single:MNIST}{{\caption@xref {fig:single:MNIST}{ on input line 15}}{3}}
\newnamelabel{fig:single:MNIST}{{MNIST}{MNIST}}
\newlabel{fig:single:USPS}{{\caption@xref {fig:single:USPS}{ on input line 17}}{47}}
\newsmartlabel{fig:single:USPS}{{\caption@xref {fig:single:USPS}{ on input line 17}}{3}}
\newnamelabel{fig:single:USPS}{{MNIST}{MNIST}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Some examples of MNIST \& USPS.\relax }}{47}}
\newlabel{fig:single:dataset}{{3.7}{47}}
\newsmartlabel{fig:single:dataset}{{3.7}{3}}
\newnamelabel{fig:single:dataset}{{MNIST}{MNIST}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {MNIST}}}{47}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {USPS}}}{47}}
\newlabel{tab:single:MNIST}{{\caption@xref {tab:single:MNIST}{ on input line 22}}{47}}
\newsmartlabel{tab:single:MNIST}{{\caption@xref {tab:single:MNIST}{ on input line 22}}{3}}
\newnamelabel{tab:single:MNIST}{{MNIST}{MNIST}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Data distribution of MNIST subset\relax }}{47}}
\newlabel{tab:addlabel}{{3.2}{47}}
\newsmartlabel{tab:addlabel}{{3.2}{3}}
\newnamelabel{tab:addlabel}{{MNIST}{MNIST}}
\@writefile{toc}{\contentsline {subsubsection}{USPS}{47}}
\citation{ben2010theory}
\newlabel{fig:single:split}{{\caption@xref {fig:single:split}{ on input line 51}}{48}}
\newsmartlabel{fig:single:split}{{\caption@xref {fig:single:split}{ on input line 51}}{3}}
\newnamelabel{fig:single:split}{{Experiment Setup}{Experiment Setup}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Illustration of our training procedure. The data is splitted into 3 sets: a source training set, a small target training set and a large test set. Salt \& pepper noise is added into the source training set in order to generate different source models.\relax }}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Experiment Setup}{48}}
\newlabel{fig:single:split}{{\caption@xref {fig:single:split}{ on input line 57}}{49}}
\newsmartlabel{fig:single:split}{{\caption@xref {fig:single:split}{ on input line 57}}{3}}
\newnamelabel{fig:single:split}{{Experiment Setup}{Experiment Setup}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Images with different level of noise rate. By adding more slat\&pepper noise, the images become less clear.\relax }}{49}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Noise Rate = 0 }}}{49}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Noise Rate = 0.3}}}{49}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Noise Rate = 0.5}}}{49}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Noise Rate = 0.8}}}{49}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Setups for our experiment on two datasets\relax }}{49}}
\newlabel{tab:addlabel}{{3.3}{49}}
\newsmartlabel{tab:addlabel}{{3.3}{3}}
\newnamelabel{tab:addlabel}{{Experiment Setup}{Experiment Setup}}
\@setckpt{transfer/total}{
\setcounter{page}{50}
\setcounter{equation}{38}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{7}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{9}
\setcounter{table}{3}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{parentequation}{0}
\setcounter{@smartlistlen}{2}
\setcounter{less@smartlist}{0}
\setcounter{@currsmartlistplace}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{4}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lips@count}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{1}
\setcounter{ALC@unique}{15}
\setcounter{ALC@line}{15}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{defi}{2}
\setcounter{theorem}{0}
\setcounter{myappendices}{0}
\setcounter{appdepth}{1}
}
